{
    "docs": [
        {
            "location": "/",
            "text": "Tree-based Pipeline Optimization Tool (TPOT)\n\n\nConsider TPOT your \nData Science Assistant\n. TPOT is a Python tool that automatically creates and optimizes Machine Learning pipelines using genetic programming.\n\n\nTPOT will automate the most tedious part of Machine Learning by intelligently exploring thousands of possible pipelines to find the best one for your data.\n\n\n\n\n\n\nAn example Machine Learning pipeline\n\n\n\n\nOnce TPOT is finished searching (or you get tired of waiting), it provides you with the Python code for the best pipeline it found so you can tinker with the pipeline from there.\n\n\n\n\n\n\nAn example TPOT pipeline\n\n\n\n\nTPOT is built on top of scikit-learn, so all of the code it generates should look familiar... if you're familiar with scikit-learn, anyway.\n\n\nTPOT is still under active development\n and we encourage you to check back on this repository regularly for updates.\n\n\nLicense\n\n\nPlease see the \nrepository license\n for the licensing and usage information for TPOT.",
            "title": "Home"
        },
        {
            "location": "/#tree-based-pipeline-optimization-tool-tpot",
            "text": "Consider TPOT your  Data Science Assistant . TPOT is a Python tool that automatically creates and optimizes Machine Learning pipelines using genetic programming.  TPOT will automate the most tedious part of Machine Learning by intelligently exploring thousands of possible pipelines to find the best one for your data.    An example Machine Learning pipeline   Once TPOT is finished searching (or you get tired of waiting), it provides you with the Python code for the best pipeline it found so you can tinker with the pipeline from there.    An example TPOT pipeline   TPOT is built on top of scikit-learn, so all of the code it generates should look familiar... if you're familiar with scikit-learn, anyway.  TPOT is still under active development  and we encourage you to check back on this repository regularly for updates.",
            "title": "Tree-based Pipeline Optimization Tool (TPOT)"
        },
        {
            "location": "/#license",
            "text": "Please see the  repository license  for the licensing and usage information for TPOT.",
            "title": "License"
        },
        {
            "location": "/examples/Using_TPOT_via_the_command_line/",
            "text": "Using TPOT via the command line\n\n\nTo use TPOT via the command line, enter the following command to see the parameters that TPOT can receive:\n\n\ntpot --help\n\n\n\n\nThe following parameters will display along with their descriptions:\n\n\n\n\n-i\n / \nINPUT_FILE\n: The path to the data file to optimize the pipeline on. Make sure that the class column in the file is labeled as \"class\".\n\n\n-is\n / \nINPUT_SEPARATOR\n: The character used to separate columns in the input file. Commas (,) and tabs (\\t) are the most common separators.\n\n\n-o\n / \nOUTPUT_FILE\n: The path to a file that you wish to export the pipeline code into. By default, exporting is disabled.\n\n\n-g\n / \nGENERATIONS\n: The number of generations to run pipeline optimization for. Must be \n 0. The more generations you give TPOT to run, the longer it takes, but it's also more likely to find better pipelines.\n\n\n-p\n / \nPOPULATION\n: The number of pipelines in the genetic algorithm population. Must be \n 0. The more pipelines in the population, the slower TPOT will run, but it's also more likely to find better pipelines.\n\n\n-mr\n / \nMUTATION_RATE\n: The mutation rate for the genetic programming algorithm in the range [0.0, 1.0]. This tells the genetic programming algorithm how many pipelines to apply random changes to every generation. We don't recommend that you tweak this parameter unless you know what you're doing.\n\n\n-xr\n / \nCROSSOVER_RATE\n: The crossover rate for the genetic programming algorithm in the range [0.0, 1.0]. This tells the genetic programming algorithm how many pipelines to \"breed\" every generation. We don't recommend that you tweak this parameter unless you know what you're doing.\n\n\n-s\n / \nRANDOM_STATE\n: The random number generator seed for TPOT. Use this to make sure that TPOT will give you the same results each time you run it against the same data set with that seed.\n\n\n-v\n / \nVERBOSITY\n: How much information TPOT communicates while it's running. 0 = none, 1 = minimal, 2 = all\n\n\n\n\nAn example command-line call to TPOT may look like:\n\n\ntpot -i data/mnist.csv -is , -o tpot_exported_pipeline.py -g 100 -s 42 -v 2",
            "title": "Using TPOT via the command line"
        },
        {
            "location": "/examples/Using_TPOT_via_the_command_line/#using-tpot-via-the-command-line",
            "text": "To use TPOT via the command line, enter the following command to see the parameters that TPOT can receive:  tpot --help  The following parameters will display along with their descriptions:   -i  /  INPUT_FILE : The path to the data file to optimize the pipeline on. Make sure that the class column in the file is labeled as \"class\".  -is  /  INPUT_SEPARATOR : The character used to separate columns in the input file. Commas (,) and tabs (\\t) are the most common separators.  -o  /  OUTPUT_FILE : The path to a file that you wish to export the pipeline code into. By default, exporting is disabled.  -g  /  GENERATIONS : The number of generations to run pipeline optimization for. Must be   0. The more generations you give TPOT to run, the longer it takes, but it's also more likely to find better pipelines.  -p  /  POPULATION : The number of pipelines in the genetic algorithm population. Must be   0. The more pipelines in the population, the slower TPOT will run, but it's also more likely to find better pipelines.  -mr  /  MUTATION_RATE : The mutation rate for the genetic programming algorithm in the range [0.0, 1.0]. This tells the genetic programming algorithm how many pipelines to apply random changes to every generation. We don't recommend that you tweak this parameter unless you know what you're doing.  -xr  /  CROSSOVER_RATE : The crossover rate for the genetic programming algorithm in the range [0.0, 1.0]. This tells the genetic programming algorithm how many pipelines to \"breed\" every generation. We don't recommend that you tweak this parameter unless you know what you're doing.  -s  /  RANDOM_STATE : The random number generator seed for TPOT. Use this to make sure that TPOT will give you the same results each time you run it against the same data set with that seed.  -v  /  VERBOSITY : How much information TPOT communicates while it's running. 0 = none, 1 = minimal, 2 = all   An example command-line call to TPOT may look like:  tpot -i data/mnist.csv -is , -o tpot_exported_pipeline.py -g 100 -s 42 -v 2",
            "title": "Using TPOT via the command line"
        },
        {
            "location": "/examples/Using_TPOT_via_code/",
            "text": "Using TPOT via code\n\n\nWe've taken care to design the TPOT interface to be as similar as possible to scikit-learn.\n\n\nTPOT can be imported just like any regular Python module. To import TPOT, type:\n\n\nfrom tpot import TPOT\n\n\n\n\nthen create an instance of TPOT as follows:\n\n\nfrom tpot import TPOT\n\npipeline_optimizer = TPOT()\n\n\n\n\nNote that you can pass several parameters to the TPOT instantiation call:\n\n\n\n\ngenerations\n: The number of generations to run pipeline optimization for. Must be \n 0. The more generations you give TPOT to run, the longer it takes, but it's also more likely to find better pipelines.\n\n\npopulation_size\n: The number of pipelines in the genetic algorithm population. Must be \n 0. The more pipelines in the population, the slower TPOT will run, but it's also more likely to find better pipelines.\n\n\nmutation_rate\n: The mutation rate for the genetic programming algorithm in the range [0.0, 1.0]. This tells the genetic programming algorithm how many pipelines to apply random changes to every generation. We don't recommend that you tweak this parameter unless you know what you're doing.\n\n\ncrossover_rate\n: The crossover rate for the genetic programming algorithm in the range [0.0, 1.0]. This tells the genetic programming algorithm how many pipelines to \"breed\" every generation. We don't recommend that you tweak this parameter unless you know what you're doing.\n\n\nrandom_state\n: The random number generator seed for TPOT. Use this to make sure that TPOT will give you the same results each time you run it against the same data set with that seed.\n\n\nverbosity\n: How much information TPOT communicates while it's running. 0 = none, 1 = minimal, 2 = all\n\n\n\n\nSome example code with custom TPOT parameters might look like:\n\n\nfrom tpot import TPOT\n\npipeline_optimizer = TPOT(generations=100, random_state=42, verbosity=2)\n\n\n\n\nNow TPOT is ready to work! You can tell TPOT to optimize a pipeline based on a data set with the \nfit\n function:\n\n\nfrom tpot import TPOT\n\npipeline_optimizer = TPOT(generations=100, random_state=42, verbosity=2)\npipeline_optimizer.fit(training_features, training_classes)\n\n\n\n\nthen evaluate the final pipeline with the \nscore()\n function:\n\n\nfrom tpot import TPOT\n\npipeline_optimizer = TPOT(generations=100, random_state=42, verbosity=2)\npipeline_optimizer.fit(training_features, training_classes)\nprint(pipeline_optimizer.score(training_features, training_classes, testing_features, testing_classes))\n\n\n\n\nNote that you need to pass the training data to the \nscore()\n function so the pipeline re-trains the scikit-learn models on the training data.\n\n\nFinally, you can tell TPOT to export the optimized pipeline to a text file with the \nexport()\n function:\n\n\nfrom tpot import TPOT\n\npipeline_optimizer = TPOT(generations=100, random_state=42, verbosity=2)\npipeline_optimizer.fit(training_features, training_classes)\nprint(pipeline_optimizer.score(training_features, training_classes, testing_features, testing_classes))\npipeline_optimizer.export('tpot_exported_pipeline.py')\n\n\n\n\nOnce this code finishes running, \ntpot_exported_pipeline.py\n will contain the Python code for the optimized pipeline.",
            "title": "Using TPOT via code"
        },
        {
            "location": "/examples/Using_TPOT_via_code/#using-tpot-via-code",
            "text": "We've taken care to design the TPOT interface to be as similar as possible to scikit-learn.  TPOT can be imported just like any regular Python module. To import TPOT, type:  from tpot import TPOT  then create an instance of TPOT as follows:  from tpot import TPOT\n\npipeline_optimizer = TPOT()  Note that you can pass several parameters to the TPOT instantiation call:   generations : The number of generations to run pipeline optimization for. Must be   0. The more generations you give TPOT to run, the longer it takes, but it's also more likely to find better pipelines.  population_size : The number of pipelines in the genetic algorithm population. Must be   0. The more pipelines in the population, the slower TPOT will run, but it's also more likely to find better pipelines.  mutation_rate : The mutation rate for the genetic programming algorithm in the range [0.0, 1.0]. This tells the genetic programming algorithm how many pipelines to apply random changes to every generation. We don't recommend that you tweak this parameter unless you know what you're doing.  crossover_rate : The crossover rate for the genetic programming algorithm in the range [0.0, 1.0]. This tells the genetic programming algorithm how many pipelines to \"breed\" every generation. We don't recommend that you tweak this parameter unless you know what you're doing.  random_state : The random number generator seed for TPOT. Use this to make sure that TPOT will give you the same results each time you run it against the same data set with that seed.  verbosity : How much information TPOT communicates while it's running. 0 = none, 1 = minimal, 2 = all   Some example code with custom TPOT parameters might look like:  from tpot import TPOT\n\npipeline_optimizer = TPOT(generations=100, random_state=42, verbosity=2)  Now TPOT is ready to work! You can tell TPOT to optimize a pipeline based on a data set with the  fit  function:  from tpot import TPOT\n\npipeline_optimizer = TPOT(generations=100, random_state=42, verbosity=2)\npipeline_optimizer.fit(training_features, training_classes)  then evaluate the final pipeline with the  score()  function:  from tpot import TPOT\n\npipeline_optimizer = TPOT(generations=100, random_state=42, verbosity=2)\npipeline_optimizer.fit(training_features, training_classes)\nprint(pipeline_optimizer.score(training_features, training_classes, testing_features, testing_classes))  Note that you need to pass the training data to the  score()  function so the pipeline re-trains the scikit-learn models on the training data.  Finally, you can tell TPOT to export the optimized pipeline to a text file with the  export()  function:  from tpot import TPOT\n\npipeline_optimizer = TPOT(generations=100, random_state=42, verbosity=2)\npipeline_optimizer.fit(training_features, training_classes)\nprint(pipeline_optimizer.score(training_features, training_classes, testing_features, testing_classes))\npipeline_optimizer.export('tpot_exported_pipeline.py')  Once this code finishes running,  tpot_exported_pipeline.py  will contain the Python code for the optimized pipeline.",
            "title": "Using TPOT via code"
        },
        {
            "location": "/examples/MNIST_Example/",
            "text": "MNIST Example\n\n\nBelow is a minimal working example with the practice MNIST data set.\n\n\nfrom tpot import TPOT\nfrom sklearn.datasets import load_digits\nfrom sklearn.cross_validation import train_test_split\n\ndigits = load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n                                                    train_size=0.75)\n\ntpot = TPOT(generations=5)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_train, y_train, X_test, y_test))\ntpot.export('tpot_mnist_pipeline.py')\n\n\n\n\nRunning this code should discover a pipeline that achieves ~97% testing accuracy, and the corresponding Python code should be exported to the \ntpot_mnist_pipeline.py\n file.",
            "title": "MNIST Example"
        },
        {
            "location": "/examples/MNIST_Example/#mnist-example",
            "text": "Below is a minimal working example with the practice MNIST data set.  from tpot import TPOT\nfrom sklearn.datasets import load_digits\nfrom sklearn.cross_validation import train_test_split\n\ndigits = load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n                                                    train_size=0.75)\n\ntpot = TPOT(generations=5)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_train, y_train, X_test, y_test))\ntpot.export('tpot_mnist_pipeline.py')  Running this code should discover a pipeline that achieves ~97% testing accuracy, and the corresponding Python code should be exported to the  tpot_mnist_pipeline.py  file.",
            "title": "MNIST Example"
        },
        {
            "location": "/changelog/",
            "text": "Changelog\n\n\nv0.1.3\n\n\n\n\nFirst Release\n\n\nDownload links: \nSource code (zip)\n, \nSource code (tar.gz)",
            "title": "Changelog"
        },
        {
            "location": "/changelog/#changelog",
            "text": "v0.1.3   First Release  Download links:  Source code (zip) ,  Source code (tar.gz)",
            "title": "Changelog"
        },
        {
            "location": "/installing/",
            "text": "Installation\n\n\nTPOT is built on top of several existing Python libraries, including:\n\n\n\n\n\n\nNumPy\n\n\n\n\n\n\nSciPy\n\n\n\n\n\n\npandas\n\n\n\n\n\n\nscikit-learn\n\n\n\n\n\n\nDEAP\n\n\n\n\n\n\nExcept for DEAP, all of the necessary Python packages can be installed via the \nAnaconda Python distribution\n, which we strongly recommend that you use. We also strongly recommend that you use of Python 3 over Python 2 if you're given the choice.\n\n\nNumPy, SciPy, pandas, and scikit-learn can be installed in Anaconda via the command:\n\n\nconda install numpy scipy pandas scikit-learn\n\n\n\n\nDEAP can be installed with \npip\n via the command:\n\n\npip install deap\n\n\n\n\nFinally to install TPOT, run the following command:\n\n\npip install tpot\n\n\n\n\nPlease \nfile a new issue\n if you run into installation problems.",
            "title": "Installation"
        },
        {
            "location": "/installing/#installation",
            "text": "TPOT is built on top of several existing Python libraries, including:    NumPy    SciPy    pandas    scikit-learn    DEAP    Except for DEAP, all of the necessary Python packages can be installed via the  Anaconda Python distribution , which we strongly recommend that you use. We also strongly recommend that you use of Python 3 over Python 2 if you're given the choice.  NumPy, SciPy, pandas, and scikit-learn can be installed in Anaconda via the command:  conda install numpy scipy pandas scikit-learn  DEAP can be installed with  pip  via the command:  pip install deap  Finally to install TPOT, run the following command:  pip install tpot  Please  file a new issue  if you run into installation problems.",
            "title": "Installation"
        },
        {
            "location": "/contributing/",
            "text": "Contributing\n\n\nWe welcome you to \ncheck the existing issues\n for bugs or enhancements to work on. If you have an idea for an extension to TPOT, please \nfile a new issue\n so we can discuss it.",
            "title": "Contributing"
        },
        {
            "location": "/contributing/#contributing",
            "text": "We welcome you to  check the existing issues  for bugs or enhancements to work on. If you have an idea for an extension to TPOT, please  file a new issue  so we can discuss it.",
            "title": "Contributing"
        }
    ]
}