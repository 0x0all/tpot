{
    "docs": [
        {
            "location": "/",
            "text": "Tree-based Pipeline Optimization Tool (TPOT)\n\n\n\n\n\n\n\n\nConsider TPOT your \nData Science Assistant\n. TPOT is a Python tool that automatically creates and optimizes Machine Learning pipelines using genetic programming.\n\n\nTPOT will automate the most tedious part of Machine Learning by intelligently exploring thousands of possible pipelines to find the best one for your data.\n\n\n\n\n\n\nAn example Machine Learning pipeline\n\n\n\n\nOnce TPOT is finished searching (or you get tired of waiting), it provides you with the Python code for the best pipeline it found so you can tinker with the pipeline from there.\n\n\n\n\n\n\nAn example TPOT pipeline\n\n\n\n\nTPOT is built on top of scikit-learn, so all of the code it generates should look familiar... if you're familiar with scikit-learn, anyway.\n\n\nTPOT is still under active development\n and we encourage you to check back on this repository regularly for updates.\n\n\nLicense\n\n\nPlease see the \nrepository license\n for the licensing and usage information for TPOT.",
            "title": "Home"
        },
        {
            "location": "/#tree-based-pipeline-optimization-tool-tpot",
            "text": "Consider TPOT your  Data Science Assistant . TPOT is a Python tool that automatically creates and optimizes Machine Learning pipelines using genetic programming.  TPOT will automate the most tedious part of Machine Learning by intelligently exploring thousands of possible pipelines to find the best one for your data.    An example Machine Learning pipeline   Once TPOT is finished searching (or you get tired of waiting), it provides you with the Python code for the best pipeline it found so you can tinker with the pipeline from there.    An example TPOT pipeline   TPOT is built on top of scikit-learn, so all of the code it generates should look familiar... if you're familiar with scikit-learn, anyway.  TPOT is still under active development  and we encourage you to check back on this repository regularly for updates.",
            "title": "Tree-based Pipeline Optimization Tool (TPOT)"
        },
        {
            "location": "/#license",
            "text": "Please see the  repository license  for the licensing and usage information for TPOT.",
            "title": "License"
        },
        {
            "location": "/installing/",
            "text": "Installation\n\n\nTPOT is built on top of several existing Python libraries, including:\n\n\n\n\n\n\nNumPy\n\n\n\n\n\n\nSciPy\n\n\n\n\n\n\npandas\n\n\n\n\n\n\nscikit-learn\n\n\n\n\n\n\nDEAP\n\n\n\n\n\n\nXGBoost\n\n\n\n\n\n\nExcept for DEAP, all of the necessary Python packages can be installed via the \nAnaconda Python distribution\n, which we strongly recommend that you use. We also strongly recommend that you use of Python 3 over Python 2 if you're given the choice.\n\n\nNumPy, SciPy, pandas, and scikit-learn can be installed in Anaconda via the command:\n\n\nconda install numpy scipy pandas scikit-learn\n\n\n\n\nDEAP and XGBoost can be installed with \npip\n via the command:\n\n\npip install deap xgboost\n\n\n\n\nOptional: For OS X users who want to use OpenMP-enabled compilers to install XGBoost, gcc-5.x.x can be installed with Homebrew: \nbrew install gcc --without-multilib\n.\n\n\nFinally to install TPOT itself, run the following command:\n\n\npip install tpot\n\n\n\n\nPlease \nfile a new issue\n if you run into installation problems.",
            "title": "Installation"
        },
        {
            "location": "/installing/#installation",
            "text": "TPOT is built on top of several existing Python libraries, including:    NumPy    SciPy    pandas    scikit-learn    DEAP    XGBoost    Except for DEAP, all of the necessary Python packages can be installed via the  Anaconda Python distribution , which we strongly recommend that you use. We also strongly recommend that you use of Python 3 over Python 2 if you're given the choice.  NumPy, SciPy, pandas, and scikit-learn can be installed in Anaconda via the command:  conda install numpy scipy pandas scikit-learn  DEAP and XGBoost can be installed with  pip  via the command:  pip install deap xgboost  Optional: For OS X users who want to use OpenMP-enabled compilers to install XGBoost, gcc-5.x.x can be installed with Homebrew:  brew install gcc --without-multilib .  Finally to install TPOT itself, run the following command:  pip install tpot  Please  file a new issue  if you run into installation problems.",
            "title": "Installation"
        },
        {
            "location": "/contributing/",
            "text": "Contributing\n\n\nWe welcome you to \ncheck the existing issues\n for bugs or enhancements to work on. If you have an idea for an extension to TPOT, please \nfile a new issue\n so we can discuss it.\n\n\nHow to contribute\n\n\nThe preferred way to contribute to TPOT is to fork the \n\nmain repository\n on\nGitHub:\n\n\n\n\n\n\nFork the \nproject repository\n:\n   click on the 'Fork' button near the top of the page. This creates\n   a copy of the code under your account on the GitHub server.\n\n\n\n\n\n\nClone this copy to your local disk:\n\n\n  $ git clone git@github.com:YourLogin/tpot.git\n  $ cd tpot\n\n\n\n\n\n\n\nCreate a branch to hold your changes:\n\n\n  $ git checkout -b my-contribution\n\n\n\n\n\n\n\nand start making changes. Never work in the \nmaster\n branch!\n\n\n\n\nWork on this copy on your computer using Git to do the version\n   control. When you're done editing, do:\n  $ git add modified_files\n  $ git commit\n\n\n\n\n\n\n\nto record your changes in Git, then push them to GitHub with:\n\n\n      $ git push -u origin my-contribution\n\n\n\nFinally, go to the web page of your fork of the TPOT repo,\nand click 'Pull Request' (PR) to send your changes to the maintainers for\nreview. This will send an email to the maintainers.\n\n\n(If any of the above seems like magic to you, then look up the \n\nGit documentation\n on the web.)\n\n\nBefore submitting your pull request\n\n\nBefore you submit a pull request for your contribution, please work through this checklist to make sure that you have done everything necessary so we can efficiently review and accept your changes.\n\n\nIf your contribution changes TPOT in any way:\n\n\n\n\n\n\nUpdate the \ndocumentation\n so all of your changes are reflected there.\n\n\n\n\n\n\nUpdate the \nREADME\n if anything there has changed.\n\n\n\n\n\n\nIf your contribution involves any code changes:\n\n\n\n\n\n\nUpdate the \nproject unit tests\n to test your code changes.\n\n\n\n\n\n\nMake sure that your code is properly commented with \ndocstrings\n and comments explaining your rationale behind non-obvious coding practices.\n\n\n\n\n\n\nIf your contribution requires a new library dependency outside of DEAP and scikit-learn:\n\n\n\n\n\n\nDouble-check that the new dependency is easy to install via \npip\n or Anaconda and supports both Python 2 and 3. If the dependency requires a complicated installation, then we most likely won't merge your changes because we want to keep TPOT easy to install.\n\n\n\n\n\n\nAdd the required version of the library to \n.travis.yml\n\n\n\n\n\n\nAdd a line to pip install the library to \n.travis_install.sh\n\n\n\n\n\n\nAdd a line to print the version of the library to \n.travis_install.sh\n\n\n\n\n\n\nSimilarly add a line to print the version of the library to \n.travis_test.sh\n\n\n\n\n\n\nAfter submitting your pull request\n\n\nAfter submitting your pull request, \nTravis-CI\n will automatically run unit tests on your changes and make sure that your updated code builds and runs on Python 2 and 3. We also use services that automatically check code quality and test coverage.\n\n\nCheck back shortly after submitting your pull request to make sure that your code passes these checks. If any of the checks come back with a red X, then do your best to address the errors.",
            "title": "Contributing"
        },
        {
            "location": "/contributing/#contributing",
            "text": "We welcome you to  check the existing issues  for bugs or enhancements to work on. If you have an idea for an extension to TPOT, please  file a new issue  so we can discuss it.",
            "title": "Contributing"
        },
        {
            "location": "/contributing/#how-to-contribute",
            "text": "The preferred way to contribute to TPOT is to fork the  main repository  on\nGitHub:    Fork the  project repository :\n   click on the 'Fork' button near the top of the page. This creates\n   a copy of the code under your account on the GitHub server.    Clone this copy to your local disk:    $ git clone git@github.com:YourLogin/tpot.git\n  $ cd tpot    Create a branch to hold your changes:    $ git checkout -b my-contribution    and start making changes. Never work in the  master  branch!   Work on this copy on your computer using Git to do the version\n   control. When you're done editing, do:   $ git add modified_files\n  $ git commit    to record your changes in Git, then push them to GitHub with:        $ git push -u origin my-contribution  Finally, go to the web page of your fork of the TPOT repo,\nand click 'Pull Request' (PR) to send your changes to the maintainers for\nreview. This will send an email to the maintainers.  (If any of the above seems like magic to you, then look up the  Git documentation  on the web.)",
            "title": "How to contribute"
        },
        {
            "location": "/contributing/#before-submitting-your-pull-request",
            "text": "Before you submit a pull request for your contribution, please work through this checklist to make sure that you have done everything necessary so we can efficiently review and accept your changes.  If your contribution changes TPOT in any way:    Update the  documentation  so all of your changes are reflected there.    Update the  README  if anything there has changed.    If your contribution involves any code changes:    Update the  project unit tests  to test your code changes.    Make sure that your code is properly commented with  docstrings  and comments explaining your rationale behind non-obvious coding practices.    If your contribution requires a new library dependency outside of DEAP and scikit-learn:    Double-check that the new dependency is easy to install via  pip  or Anaconda and supports both Python 2 and 3. If the dependency requires a complicated installation, then we most likely won't merge your changes because we want to keep TPOT easy to install.    Add the required version of the library to  .travis.yml    Add a line to pip install the library to  .travis_install.sh    Add a line to print the version of the library to  .travis_install.sh    Similarly add a line to print the version of the library to  .travis_test.sh",
            "title": "Before submitting your pull request"
        },
        {
            "location": "/contributing/#after-submitting-your-pull-request",
            "text": "After submitting your pull request,  Travis-CI  will automatically run unit tests on your changes and make sure that your updated code builds and runs on Python 2 and 3. We also use services that automatically check code quality and test coverage.  Check back shortly after submitting your pull request to make sure that your code passes these checks. If any of the checks come back with a red X, then do your best to address the errors.",
            "title": "After submitting your pull request"
        },
        {
            "location": "/examples/Using_TPOT_via_the_command_line/",
            "text": "Using TPOT via the command line\n\n\nTo use TPOT via the command line, enter the following command to see the parameters that TPOT can receive:\n\n\ntpot --help\n\n\n\n\nThe following parameters will display along with their descriptions:\n\n\n\n\n-i\n / \nINPUT_FILE\n: The path to the data file to optimize the pipeline on. Make sure that the class column in the file is labeled as \"class\".\n\n\n-is\n / \nINPUT_SEPARATOR\n: The character used to separate columns in the input file. Commas (,) and tabs (\\t) are the most common separators.\n\n\n-o\n / \nOUTPUT_FILE\n: The path to a file that you wish to export the pipeline code into. By default, exporting is disabled.\n\n\n-g\n / \nGENERATIONS\n: The number of generations to run pipeline optimization for. Must be > 0. The more generations you give TPOT to run, the longer it takes, but it's also more likely to find better pipelines.\n\n\n-p\n / \nPOPULATION\n: The number of pipelines in the genetic algorithm population. Must be > 0. The more pipelines in the population, the slower TPOT will run, but it's also more likely to find better pipelines.\n\n\n-mr\n / \nMUTATION_RATE\n: The mutation rate for the genetic programming algorithm in the range [0.0, 1.0]. This tells the genetic programming algorithm how many pipelines to apply random changes to every generation. We don't recommend that you tweak this parameter unless you know what you're doing.\n\n\n-xr\n / \nCROSSOVER_RATE\n: The crossover rate for the genetic programming algorithm in the range [0.0, 1.0]. This tells the genetic programming algorithm how many pipelines to \"breed\" every generation. We don't recommend that you tweak this parameter unless you know what you're doing.\n\n\n-s\n / \nRANDOM_STATE\n: The random number generator seed for TPOT. Use this to make sure that TPOT will give you the same results each time you run it against the same data set with that seed.\n\n\n-v\n / \nVERBOSITY\n: How much information TPOT communicates while it's running. 0 = none, 1 = minimal, 2 = all\n\n\n\n\nAn example command-line call to TPOT may look like:\n\n\ntpot -i data/mnist.csv -is , -o tpot_exported_pipeline.py -g 100 -s 42 -v 2",
            "title": "Using TPOT via the command line"
        },
        {
            "location": "/examples/Using_TPOT_via_the_command_line/#using-tpot-via-the-command-line",
            "text": "To use TPOT via the command line, enter the following command to see the parameters that TPOT can receive:  tpot --help  The following parameters will display along with their descriptions:   -i  /  INPUT_FILE : The path to the data file to optimize the pipeline on. Make sure that the class column in the file is labeled as \"class\".  -is  /  INPUT_SEPARATOR : The character used to separate columns in the input file. Commas (,) and tabs (\\t) are the most common separators.  -o  /  OUTPUT_FILE : The path to a file that you wish to export the pipeline code into. By default, exporting is disabled.  -g  /  GENERATIONS : The number of generations to run pipeline optimization for. Must be > 0. The more generations you give TPOT to run, the longer it takes, but it's also more likely to find better pipelines.  -p  /  POPULATION : The number of pipelines in the genetic algorithm population. Must be > 0. The more pipelines in the population, the slower TPOT will run, but it's also more likely to find better pipelines.  -mr  /  MUTATION_RATE : The mutation rate for the genetic programming algorithm in the range [0.0, 1.0]. This tells the genetic programming algorithm how many pipelines to apply random changes to every generation. We don't recommend that you tweak this parameter unless you know what you're doing.  -xr  /  CROSSOVER_RATE : The crossover rate for the genetic programming algorithm in the range [0.0, 1.0]. This tells the genetic programming algorithm how many pipelines to \"breed\" every generation. We don't recommend that you tweak this parameter unless you know what you're doing.  -s  /  RANDOM_STATE : The random number generator seed for TPOT. Use this to make sure that TPOT will give you the same results each time you run it against the same data set with that seed.  -v  /  VERBOSITY : How much information TPOT communicates while it's running. 0 = none, 1 = minimal, 2 = all   An example command-line call to TPOT may look like:  tpot -i data/mnist.csv -is , -o tpot_exported_pipeline.py -g 100 -s 42 -v 2",
            "title": "Using TPOT via the command line"
        },
        {
            "location": "/examples/Using_TPOT_via_code/",
            "text": "Using TPOT via code\n\n\nWe've taken care to design the TPOT interface to be as similar as possible to scikit-learn.\n\n\nTPOT can be imported just like any regular Python module. To import TPOT, type:\n\n\nfrom tpot import TPOT\n\n\n\n\nthen create an instance of TPOT as follows:\n\n\nfrom tpot import TPOT\n\npipeline_optimizer = TPOT()\n\n\n\n\nNote that you can pass several parameters to the TPOT instantiation call:\n\n\n\n\ngenerations\n: The number of generations to run pipeline optimization for. Must be > 0. The more generations you give TPOT to run, the longer it takes, but it's also more likely to find better pipelines.\n\n\npopulation_size\n: The number of pipelines in the genetic algorithm population. Must be > 0. The more pipelines in the population, the slower TPOT will run, but it's also more likely to find better pipelines.\n\n\nmutation_rate\n: The mutation rate for the genetic programming algorithm in the range [0.0, 1.0]. This tells the genetic programming algorithm how many pipelines to apply random changes to every generation. We don't recommend that you tweak this parameter unless you know what you're doing.\n\n\ncrossover_rate\n: The crossover rate for the genetic programming algorithm in the range [0.0, 1.0]. This tells the genetic programming algorithm how many pipelines to \"breed\" every generation. We don't recommend that you tweak this parameter unless you know what you're doing.\n\n\nrandom_state\n: The random number generator seed for TPOT. Use this to make sure that TPOT will give you the same results each time you run it against the same data set with that seed.\n\n\nverbosity\n: How much information TPOT communicates while it's running. 0 = none, 1 = minimal, 2 = all\n\n\n\n\nSome example code with custom TPOT parameters might look like:\n\n\nfrom tpot import TPOT\n\npipeline_optimizer = TPOT(generations=100, random_state=42, verbosity=2)\n\n\n\n\nNow TPOT is ready to work! You can tell TPOT to optimize a pipeline based on a data set with the \nfit\n function:\n\n\nfrom tpot import TPOT\n\npipeline_optimizer = TPOT(generations=100, random_state=42, verbosity=2)\npipeline_optimizer.fit(training_features, training_classes)\n\n\n\n\nThe \nfit()\n function takes in a training data set, then further divides it into a training and validation data set (so as to do cross-validation). It then initializes the Genetic Algoritm to find the best pipeline based on the validation set performance evaluated on the basis of a scoring function (generally the classification accuracy, but can be user defined as well like precision/recall/f1, etc).   \n\n\nYou can then proceed to evaluate the final pipeline on the test set with the \nscore()\n function:\n\n\nfrom tpot import TPOT\n\npipeline_optimizer = TPOT(generations=100, random_state=42, verbosity=2)\npipeline_optimizer.fit(training_features, training_classes)\nprint(pipeline_optimizer.score(training_features, training_classes, testing_features, testing_classes))\n\n\n\n\nNote that you need to pass the training data to the \nscore()\n function as well so that TPOT re-trains on the training data using the optimized pipeline (consisting of feature selection operators and scikit-learn algorithms as found by \nfit()\n).\n\n\nYou also have the option to pass a user-defined scoring function to \nscore()\n. For more information on this functionality, check \nhere\n. \n\n\nFinally, you can tell TPOT to export the corresponding Python code for the optimized pipeline to a text file with the \nexport()\n function:\n\n\nfrom tpot import TPOT\n\npipeline_optimizer = TPOT(generations=100, random_state=42, verbosity=2)\npipeline_optimizer.fit(training_features, training_classes)\nprint(pipeline_optimizer.score(training_features, training_classes, testing_features, testing_classes))\npipeline_optimizer.export('tpot_exported_pipeline.py')\n\n\n\n\nOnce this code finishes running, \ntpot_exported_pipeline.py\n will contain the Python code for the optimized pipeline.",
            "title": "Using TPOT via code"
        },
        {
            "location": "/examples/Using_TPOT_via_code/#using-tpot-via-code",
            "text": "We've taken care to design the TPOT interface to be as similar as possible to scikit-learn.  TPOT can be imported just like any regular Python module. To import TPOT, type:  from tpot import TPOT  then create an instance of TPOT as follows:  from tpot import TPOT\n\npipeline_optimizer = TPOT()  Note that you can pass several parameters to the TPOT instantiation call:   generations : The number of generations to run pipeline optimization for. Must be > 0. The more generations you give TPOT to run, the longer it takes, but it's also more likely to find better pipelines.  population_size : The number of pipelines in the genetic algorithm population. Must be > 0. The more pipelines in the population, the slower TPOT will run, but it's also more likely to find better pipelines.  mutation_rate : The mutation rate for the genetic programming algorithm in the range [0.0, 1.0]. This tells the genetic programming algorithm how many pipelines to apply random changes to every generation. We don't recommend that you tweak this parameter unless you know what you're doing.  crossover_rate : The crossover rate for the genetic programming algorithm in the range [0.0, 1.0]. This tells the genetic programming algorithm how many pipelines to \"breed\" every generation. We don't recommend that you tweak this parameter unless you know what you're doing.  random_state : The random number generator seed for TPOT. Use this to make sure that TPOT will give you the same results each time you run it against the same data set with that seed.  verbosity : How much information TPOT communicates while it's running. 0 = none, 1 = minimal, 2 = all   Some example code with custom TPOT parameters might look like:  from tpot import TPOT\n\npipeline_optimizer = TPOT(generations=100, random_state=42, verbosity=2)  Now TPOT is ready to work! You can tell TPOT to optimize a pipeline based on a data set with the  fit  function:  from tpot import TPOT\n\npipeline_optimizer = TPOT(generations=100, random_state=42, verbosity=2)\npipeline_optimizer.fit(training_features, training_classes)  The  fit()  function takes in a training data set, then further divides it into a training and validation data set (so as to do cross-validation). It then initializes the Genetic Algoritm to find the best pipeline based on the validation set performance evaluated on the basis of a scoring function (generally the classification accuracy, but can be user defined as well like precision/recall/f1, etc).     You can then proceed to evaluate the final pipeline on the test set with the  score()  function:  from tpot import TPOT\n\npipeline_optimizer = TPOT(generations=100, random_state=42, verbosity=2)\npipeline_optimizer.fit(training_features, training_classes)\nprint(pipeline_optimizer.score(training_features, training_classes, testing_features, testing_classes))  Note that you need to pass the training data to the  score()  function as well so that TPOT re-trains on the training data using the optimized pipeline (consisting of feature selection operators and scikit-learn algorithms as found by  fit() ).  You also have the option to pass a user-defined scoring function to  score() . For more information on this functionality, check  here .   Finally, you can tell TPOT to export the corresponding Python code for the optimized pipeline to a text file with the  export()  function:  from tpot import TPOT\n\npipeline_optimizer = TPOT(generations=100, random_state=42, verbosity=2)\npipeline_optimizer.fit(training_features, training_classes)\nprint(pipeline_optimizer.score(training_features, training_classes, testing_features, testing_classes))\npipeline_optimizer.export('tpot_exported_pipeline.py')  Once this code finishes running,  tpot_exported_pipeline.py  will contain the Python code for the optimized pipeline.",
            "title": "Using TPOT via code"
        },
        {
            "location": "/examples/MNIST_Example/",
            "text": "MNIST Example\n\n\nBelow is a minimal working example with the practice MNIST data set.\n\n\nfrom tpot import TPOT\nfrom sklearn.datasets import load_digits\nfrom sklearn.cross_validation import train_test_split\n\ndigits = load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n                                                    train_size=0.75, test_size = 0.25)\n\ntpot = TPOT(generations=5)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_train, y_train, X_test, y_test))\ntpot.export('tpot_mnist_pipeline.py')\n\n\n\n\nRunning this code should discover a pipeline that achieves ~97% testing accuracy. Please note that sometimes when both train_size and test_size aren't specified in train_test_split() calls, the split doesn't use the entire data set. So we need to specify both.  \n\n\nFor details on how the \nfit()\n, \nscore()\n and \nexport()\n functions work, please see \nhere\n\n\nAfter running the above code, the corresponding Python code should be exported to the \ntpot_mnist_pipeline.py\n file and look similar to the following:\n\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom itertools import combinations\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indeces, testing_indeces = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size = 0.25)))\n\n\nresult1 = tpot_data.copy()\n\n# Perform classification with a logistic regression classifier\nlrc1 = LogisticRegression(C=0.1)\nlrc1.fit(result1.loc[training_indeces].drop('class', axis=1).values, result1.loc[training_indeces, 'class'].values)\nresult1['lrc1-classification'] = lrc1.predict(result1.drop('class', axis=1).values)\n\n# Decision-tree based feature selection\ntraining_features = result1.loc[training_indeces].drop('class', axis=1)\ntraining_class_vals = result1.loc[training_indeces, 'class'].values\n\npair_scores = dict()\nfor features in combinations(training_features.columns.values, 2):\n    dtc = DecisionTreeClassifier()\n    training_feature_vals = training_features[list(features)].values\n    dtc.fit(training_feature_vals, training_class_vals)\n    pair_scores[features] = (dtc.score(training_feature_vals, training_class_vals), list(features))\n\nbest_pairs = []\nfor pair in sorted(pair_scores, key=pair_scores.get, reverse=True)[:3870]:\n    best_pairs.extend(list(pair))\nbest_pairs = sorted(list(set(best_pairs)))\n\nresult2 = result1[sorted(list(set(best_pairs + ['class'])))]\n\n# Perform classification with a random forest classifier\nrfc3 = RandomForestClassifier(n_estimators=1, max_features=min(64, len(result2.columns) - 1))\nrfc3.fit(result2.loc[training_indeces].drop('class', axis=1).values, result2.loc[training_indeces, 'class'].values)\nresult3 = result2\nresult3['rfc3-classification'] = rfc3.predict(result3.drop('class', axis=1).values)\n\n# Perform classification with a decision tree classifier\ndtc4 = DecisionTreeClassifier(max_features=min(40, len(result3.columns) - 1), max_depth=7)\ndtc4.fit(result3.loc[training_indeces].drop('class', axis=1).values, result3.loc[training_indeces, 'class'].values)\nresult4 = result3\nresult4['dtc4-classification'] = dtc4.predict(result4.drop('class', axis=1).values)\n\n# Perform classification with a k-nearest neighbor classifier\nknnc5 = KNeighborsClassifier(n_neighbors=1)\nknnc5.fit(result4.loc[training_indeces].drop('class', axis=1).values, result4.loc[training_indeces, 'class'].values)\nresult5 = result4\nresult5['knnc5-classification'] = knnc5.predict(result5.drop('class', axis=1).values)",
            "title": "MNIST Example"
        },
        {
            "location": "/examples/MNIST_Example/#mnist-example",
            "text": "Below is a minimal working example with the practice MNIST data set.  from tpot import TPOT\nfrom sklearn.datasets import load_digits\nfrom sklearn.cross_validation import train_test_split\n\ndigits = load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n                                                    train_size=0.75, test_size = 0.25)\n\ntpot = TPOT(generations=5)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_train, y_train, X_test, y_test))\ntpot.export('tpot_mnist_pipeline.py')  Running this code should discover a pipeline that achieves ~97% testing accuracy. Please note that sometimes when both train_size and test_size aren't specified in train_test_split() calls, the split doesn't use the entire data set. So we need to specify both.    For details on how the  fit() ,  score()  and  export()  functions work, please see  here  After running the above code, the corresponding Python code should be exported to the  tpot_mnist_pipeline.py  file and look similar to the following:  import numpy as np\nimport pandas as pd\n\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom itertools import combinations\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indeces, testing_indeces = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size = 0.25)))\n\n\nresult1 = tpot_data.copy()\n\n# Perform classification with a logistic regression classifier\nlrc1 = LogisticRegression(C=0.1)\nlrc1.fit(result1.loc[training_indeces].drop('class', axis=1).values, result1.loc[training_indeces, 'class'].values)\nresult1['lrc1-classification'] = lrc1.predict(result1.drop('class', axis=1).values)\n\n# Decision-tree based feature selection\ntraining_features = result1.loc[training_indeces].drop('class', axis=1)\ntraining_class_vals = result1.loc[training_indeces, 'class'].values\n\npair_scores = dict()\nfor features in combinations(training_features.columns.values, 2):\n    dtc = DecisionTreeClassifier()\n    training_feature_vals = training_features[list(features)].values\n    dtc.fit(training_feature_vals, training_class_vals)\n    pair_scores[features] = (dtc.score(training_feature_vals, training_class_vals), list(features))\n\nbest_pairs = []\nfor pair in sorted(pair_scores, key=pair_scores.get, reverse=True)[:3870]:\n    best_pairs.extend(list(pair))\nbest_pairs = sorted(list(set(best_pairs)))\n\nresult2 = result1[sorted(list(set(best_pairs + ['class'])))]\n\n# Perform classification with a random forest classifier\nrfc3 = RandomForestClassifier(n_estimators=1, max_features=min(64, len(result2.columns) - 1))\nrfc3.fit(result2.loc[training_indeces].drop('class', axis=1).values, result2.loc[training_indeces, 'class'].values)\nresult3 = result2\nresult3['rfc3-classification'] = rfc3.predict(result3.drop('class', axis=1).values)\n\n# Perform classification with a decision tree classifier\ndtc4 = DecisionTreeClassifier(max_features=min(40, len(result3.columns) - 1), max_depth=7)\ndtc4.fit(result3.loc[training_indeces].drop('class', axis=1).values, result3.loc[training_indeces, 'class'].values)\nresult4 = result3\nresult4['dtc4-classification'] = dtc4.predict(result4.drop('class', axis=1).values)\n\n# Perform classification with a k-nearest neighbor classifier\nknnc5 = KNeighborsClassifier(n_neighbors=1)\nknnc5.fit(result4.loc[training_indeces].drop('class', axis=1).values, result4.loc[training_indeces, 'class'].values)\nresult5 = result4\nresult5['knnc5-classification'] = knnc5.predict(result5.drop('class', axis=1).values)",
            "title": "MNIST Example"
        },
        {
            "location": "/examples/IRIS_Example/",
            "text": "IRIS Example\n\n\nThe following code illustrates the usage of TPOT with the IRIS data set. \n\n\nfrom tpot import TPOT\nfrom sklearn.datasets import load_iris\nfrom sklearn.cross_validation import train_test_split\n\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, train_size=0.75, test_size=0.25)\n\ntpot = TPOT(generations=5)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_train, y_train, X_test, y_test))\ntpot.export('tpot_iris_pipeline.py')\n\n\n\n\nRunning this code should discover a pipeline that achieves ~92% testing accuracy. Please note that sometimes when both train_size and test_size aren't specified in train_test_split() calls, the split doesn't use the entire data set. So we need to specify both.  \n\n\nFor details on how the \nfit()\n, \nscore()\n and \nexport()\n functions work, please see \nhere\n\n\nAfter running the above code, the corresponding Python code should be exported to the \ntpot_iris_pipeline.py\n file and look similar to the following:\n\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indeces, testing_indeces = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\nresult1 = tpot_data.copy()\n\n# Perform classification with a C-support vector classifier\nsvc1 = SVC(C=0.1)\nsvc1.fit(result1.loc[training_indeces].drop('class', axis=1).values, result1.loc[training_indeces, 'class'].values)\nresult1['svc1-classification'] = svc1.predict(result1.drop('class', axis=1).values)\n\n# Subset the data columns\nsubset_df1 = result1[sorted(result1.columns.values)[4042:5640]]\nsubset_df2 = result1[[column for column in ['class'] if column not in subset_df1.columns.values]]\nresult2 = subset_df1.join(subset_df2)\n\n# Perform classification with a k-nearest neighbor classifier\nknnc3 = KNeighborsClassifier(n_neighbors=min(131, len(training_indeces)))\nknnc3.fit(result2.loc[training_indeces].drop('class', axis=1).values, result2.loc[training_indeces, 'class'].values)\nresult3 = result2\nresult3['knnc3-classification'] = knnc3.predict(result3.drop('class', axis=1).values)",
            "title": "IRIS Example"
        },
        {
            "location": "/examples/IRIS_Example/#iris-example",
            "text": "The following code illustrates the usage of TPOT with the IRIS data set.   from tpot import TPOT\nfrom sklearn.datasets import load_iris\nfrom sklearn.cross_validation import train_test_split\n\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, train_size=0.75, test_size=0.25)\n\ntpot = TPOT(generations=5)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_train, y_train, X_test, y_test))\ntpot.export('tpot_iris_pipeline.py')  Running this code should discover a pipeline that achieves ~92% testing accuracy. Please note that sometimes when both train_size and test_size aren't specified in train_test_split() calls, the split doesn't use the entire data set. So we need to specify both.    For details on how the  fit() ,  score()  and  export()  functions work, please see  here  After running the above code, the corresponding Python code should be exported to the  tpot_iris_pipeline.py  file and look similar to the following:  import numpy as np\nimport pandas as pd\n\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indeces, testing_indeces = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\nresult1 = tpot_data.copy()\n\n# Perform classification with a C-support vector classifier\nsvc1 = SVC(C=0.1)\nsvc1.fit(result1.loc[training_indeces].drop('class', axis=1).values, result1.loc[training_indeces, 'class'].values)\nresult1['svc1-classification'] = svc1.predict(result1.drop('class', axis=1).values)\n\n# Subset the data columns\nsubset_df1 = result1[sorted(result1.columns.values)[4042:5640]]\nsubset_df2 = result1[[column for column in ['class'] if column not in subset_df1.columns.values]]\nresult2 = subset_df1.join(subset_df2)\n\n# Perform classification with a k-nearest neighbor classifier\nknnc3 = KNeighborsClassifier(n_neighbors=min(131, len(training_indeces)))\nknnc3.fit(result2.loc[training_indeces].drop('class', axis=1).values, result2.loc[training_indeces, 'class'].values)\nresult3 = result2\nresult3['knnc3-classification'] = knnc3.predict(result3.drop('class', axis=1).values)",
            "title": "IRIS Example"
        },
        {
            "location": "/examples/Titanic_Kaggle_Example/",
            "text": "Titanic Kaggle Example\n\n\nTo see the TPOT applied the Titanic Kaggle dataset, see the Jupyter notebook \nhere\n.",
            "title": "Titanic Kaggle Example"
        },
        {
            "location": "/examples/Titanic_Kaggle_Example/#titanic-kaggle-example",
            "text": "To see the TPOT applied the Titanic Kaggle dataset, see the Jupyter notebook  here .",
            "title": "Titanic Kaggle Example"
        },
        {
            "location": "/examples/Custom_Scoring_Functions/",
            "text": "Custom Scoring Functions\n\n\nBelow is a minimal working example of different scoring metrics/fitness functions used with the MNIST dataset.\n\n\n\nfrom tpot import TPOT\nfrom sklearn.datasets import load_digits\nfrom sklearn.cross_validation import train_test_split\n\ndigits = load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n                                                    train_size=0.75)\n\ndef precision(result):\n    all_class_tps = []\n    all_class_tps_fps = []\n    for this_class in all_classes:\n        #True Positives are those examples that belong to a class and whose class was guessed correctly\n        this_class_tps = len(result[(result['guess'] == this_class) \\\n            & (result['class'] == this_class)])\n        all_class_tps.append(this_class_tps)\n        #False Positives are those examples that were guessed to belong to a class \n        this_class_tps_fps = len(result[(result['guess'] == this_class) \\\n            | (result['class'] == this_class)])\n        all_class_tps_fps.append(this_class_tps_fps)\n\n    micro_avg_precision = float(np.sum(all_class_tps)) / np.sum(all_class_tps_fps)\n\n    return micro_avg_precision\n\ndef recall(result):\n    all_class_tps = []\n    all_class_tps_fns = []\n    for this_class in all_classes:\n        this_class_tps = len(result[(result['guess'] == this_class) \\\n            & (result['class'] == this_class)]) \n        #True Positives and False Negatives are those examples that belong to a specific class regardless of guess\n        this_class_tps_fns = len(result[(result['class'] == this_class)])\n        all_class_tps.append(this_class_tps)\n        all_class_tps_fns.append(this_class_tps_fns)\n\n    micro_avg_recall = float(np.sum(all_class_tps)) / np.sum(all_class_tps_fns)\n    return micro_avg_recall\n\ndef f1(result):\n    all_class_tps = []\n    all_class_tps_fps = []\n    all_class_tps_fns = []\n    for this_class in all_classes:\n        this_class_tps = len(result[(result['guess'] == this_class) \\\n            & (result['class'] == this_class)])\n        this_class_tps_fns = len(result[(result['class'] == this_class)])\n        this_class_tps_fps = len(result[(result['guess'] == this_class) \\\n            | (result['class'] == this_class)])\n        all_class_tps.append(this_class_tps)\n        all_class_tps_fps.append(this_class_tps_fps)\n        all_class_tps_fns.append(this_class_tps_fns)\n    micro_avg_precision = float(np.sum(all_class_tps)) / np.sum(all_class_tps_fps)\n    micro_avg_recall = float(np.sum(all_class_tps)) / np.sum(all_class_tps_fns)\n    micro_avg_f1 = 2 * (micro_avg_precision * micro_avg_recall) / (micro_avg_precision + micro_avg_recall)\n\n    return micro_avg_f1\n\ntpot = TPOT(generations=5)\ntpot.fit(X_train, y_train)\nprint 'acc: ', tpot.score(X_train, y_train, X_test, y_test)\n\ntpot = TPOT(generations=5, scoring_function=precision)\ntpot.fit(X_train, y_train)\nprint 'precision: ', tpot.score(X_train, y_train, X_test, y_test)\n\ntpot = TPOT(generations=5, scoring_function=recall)\ntpot.fit(X_train, y_train)\nprint 'recall: ', tpot.score(X_train, y_train, X_test, y_test)\n\ntpot = TPOT(generations=5, scoring_function=f1)\ntpot.fit(X_train, y_train)\nprint 'f1: ', tpot.score(X_train, y_train, X_test, y_test)\n\n\n\n\n\nRunning this example should discover a pipeline that achieves ~98% testing accuracy, ~93% testing precision, ~97% testing recall, and ~95% testing f1.",
            "title": "Custom Scoring Functions"
        },
        {
            "location": "/examples/Custom_Scoring_Functions/#custom-scoring-functions",
            "text": "Below is a minimal working example of different scoring metrics/fitness functions used with the MNIST dataset.  \nfrom tpot import TPOT\nfrom sklearn.datasets import load_digits\nfrom sklearn.cross_validation import train_test_split\n\ndigits = load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n                                                    train_size=0.75)\n\ndef precision(result):\n    all_class_tps = []\n    all_class_tps_fps = []\n    for this_class in all_classes:\n        #True Positives are those examples that belong to a class and whose class was guessed correctly\n        this_class_tps = len(result[(result['guess'] == this_class) \\\n            & (result['class'] == this_class)])\n        all_class_tps.append(this_class_tps)\n        #False Positives are those examples that were guessed to belong to a class \n        this_class_tps_fps = len(result[(result['guess'] == this_class) \\\n            | (result['class'] == this_class)])\n        all_class_tps_fps.append(this_class_tps_fps)\n\n    micro_avg_precision = float(np.sum(all_class_tps)) / np.sum(all_class_tps_fps)\n\n    return micro_avg_precision\n\ndef recall(result):\n    all_class_tps = []\n    all_class_tps_fns = []\n    for this_class in all_classes:\n        this_class_tps = len(result[(result['guess'] == this_class) \\\n            & (result['class'] == this_class)]) \n        #True Positives and False Negatives are those examples that belong to a specific class regardless of guess\n        this_class_tps_fns = len(result[(result['class'] == this_class)])\n        all_class_tps.append(this_class_tps)\n        all_class_tps_fns.append(this_class_tps_fns)\n\n    micro_avg_recall = float(np.sum(all_class_tps)) / np.sum(all_class_tps_fns)\n    return micro_avg_recall\n\ndef f1(result):\n    all_class_tps = []\n    all_class_tps_fps = []\n    all_class_tps_fns = []\n    for this_class in all_classes:\n        this_class_tps = len(result[(result['guess'] == this_class) \\\n            & (result['class'] == this_class)])\n        this_class_tps_fns = len(result[(result['class'] == this_class)])\n        this_class_tps_fps = len(result[(result['guess'] == this_class) \\\n            | (result['class'] == this_class)])\n        all_class_tps.append(this_class_tps)\n        all_class_tps_fps.append(this_class_tps_fps)\n        all_class_tps_fns.append(this_class_tps_fns)\n    micro_avg_precision = float(np.sum(all_class_tps)) / np.sum(all_class_tps_fps)\n    micro_avg_recall = float(np.sum(all_class_tps)) / np.sum(all_class_tps_fns)\n    micro_avg_f1 = 2 * (micro_avg_precision * micro_avg_recall) / (micro_avg_precision + micro_avg_recall)\n\n    return micro_avg_f1\n\ntpot = TPOT(generations=5)\ntpot.fit(X_train, y_train)\nprint 'acc: ', tpot.score(X_train, y_train, X_test, y_test)\n\ntpot = TPOT(generations=5, scoring_function=precision)\ntpot.fit(X_train, y_train)\nprint 'precision: ', tpot.score(X_train, y_train, X_test, y_test)\n\ntpot = TPOT(generations=5, scoring_function=recall)\ntpot.fit(X_train, y_train)\nprint 'recall: ', tpot.score(X_train, y_train, X_test, y_test)\n\ntpot = TPOT(generations=5, scoring_function=f1)\ntpot.fit(X_train, y_train)\nprint 'f1: ', tpot.score(X_train, y_train, X_test, y_test)  Running this example should discover a pipeline that achieves ~98% testing accuracy, ~93% testing precision, ~97% testing recall, and ~95% testing f1.",
            "title": "Custom Scoring Functions"
        },
        {
            "location": "/ref_index/",
            "text": "TPOT\n\n\n\n\n\n\nTPOT\n\n\n\n\nPipeline Operators\n\n\n\n\nModels\n\n\n\n\npipeline_operators.models.tree.DecisionTreeClassifier\n\n\npipeline_operators.models.ensemble.RandomForestClassifier\n\n\npipeline_operators.models.ensemble.GradientBoostingClassifier\n\n\npipeline_operators.models.svm.SVC\n\n\npipeline_operators.models.nearest_neighbors.kNeighborsClassifier\n\n\npipeline_operators.models.linear_model.LogisticRegression\n\n\n\n\nFeature Selection\n\n\n\n\npipeline_operators.feature_selection.VarianceThreshold\n\n\npipeline_operators.feature_selection.SelectKBest\n\n\npipeline_operators.feature_selection.SelectPercentile\n\n\npipeline_operators.feature_selection.RFE\n\n\n\n\nPre-processing\n\n\n\n\npipeline_operators.preprocessing.PolynomialFeatures\n\n\npipeline_operators.preprocessing.scaling.StandardScaler\n\n\npipeline_operators.preprocessing.scaling.RobustScaler\n\n\n\n\nDecomposition\n\n\n\n\npipeline_operators.decomposition.RandomizedPCA",
            "title": "Index"
        },
        {
            "location": "/ref_index/#tpot",
            "text": "TPOT",
            "title": "TPOT"
        },
        {
            "location": "/ref_index/#pipeline-operators",
            "text": "",
            "title": "Pipeline Operators"
        },
        {
            "location": "/ref_index/#models",
            "text": "pipeline_operators.models.tree.DecisionTreeClassifier  pipeline_operators.models.ensemble.RandomForestClassifier  pipeline_operators.models.ensemble.GradientBoostingClassifier  pipeline_operators.models.svm.SVC  pipeline_operators.models.nearest_neighbors.kNeighborsClassifier  pipeline_operators.models.linear_model.LogisticRegression",
            "title": "Models"
        },
        {
            "location": "/ref_index/#feature-selection",
            "text": "pipeline_operators.feature_selection.VarianceThreshold  pipeline_operators.feature_selection.SelectKBest  pipeline_operators.feature_selection.SelectPercentile  pipeline_operators.feature_selection.RFE",
            "title": "Feature Selection"
        },
        {
            "location": "/ref_index/#pre-processing",
            "text": "pipeline_operators.preprocessing.PolynomialFeatures  pipeline_operators.preprocessing.scaling.StandardScaler  pipeline_operators.preprocessing.scaling.RobustScaler",
            "title": "Pre-processing"
        },
        {
            "location": "/ref_index/#decomposition",
            "text": "pipeline_operators.decomposition.RandomizedPCA",
            "title": "Decomposition"
        },
        {
            "location": "/reference/TPOT/",
            "text": "TPOT\n\n\n\n\ntpot.fit(self,features, classes, feature_names=None)\n\n\nUses genetic programming to optimize a Machine Learning pipeline that\n   maximizes classification accuracy on the provided \nfeatures\n and \nclasses\n.\n   Optionally, name the features in the data frame according to \nfeature_names\n.\n   Performs a stratified training/testing cross-validaton split to avoid\n   overfitting on the provided data.\n\n\nParameters\n\n\nfeatures: array-like {n_samples, n_features}\n    Feature matrix\nclasses: array-like {n_samples}\n    List of class labels for prediction\nfeature_names: array-like {n_features} (default: None)\n    List of feature names as strings\n\n\n\nReturns\n\n\nNone\n\n\n\n\n\ntpot.predict(self, training_features, training_classes, testing_features)\n\n\nUses the optimized pipeline to predict the classes for a feature set.\n\n\nParameters\n\n\ntraining_features: array-like {n_samples, n_features}\n    Feature matrix of the training set\ntraining_classes: array-like {n_samples}\n    List of class labels for prediction in the training set\ntesting_features: array-like {n_samples, n_features}\n    Feature matrix of the testing set\n\n\n\nReturns\n\n\narray-like: {n_samples}\n    Predicted classes for the testing set\n\n\n\n\n\ntpot.score(self, training_features, training_classes, testing_features, testing_classes)\n\n\nEstimates the testing accuracy of the optimized pipeline.\n\n\n\nParameters\n\n\ntraining_features: array-like {n_samples, n_features}\n    Feature matrix of the training set\ntraining_classes: array-like {n_samples}\n    List of class labels for prediction in the training set\ntesting_features: array-like {n_samples, n_features}\n    Feature matrix of the testing set\ntesting_classes: array-like {n_samples}\n    List of class labels for prediction in the testing set\n\n\n\nReturns\n\n\naccuracy_score: float\n    The estimated test set accuracy\n\n\n\n\n\ntpot.export(self, output_file_name)\n \n\n\nExports the current optimized pipeline as Python code.\n\n\n\nParameters\n\n\noutput_file_name: string\n    String containing the path and file name of the desired output file\n\n\n\nReturns\n\n\nNone",
            "title": "TPOT Object"
        },
        {
            "location": "/reference/TPOT/#tpot",
            "text": "tpot.fit(self,features, classes, feature_names=None)  Uses genetic programming to optimize a Machine Learning pipeline that\n   maximizes classification accuracy on the provided  features  and  classes .\n   Optionally, name the features in the data frame according to  feature_names .\n   Performs a stratified training/testing cross-validaton split to avoid\n   overfitting on the provided data.  Parameters  features: array-like {n_samples, n_features}\n    Feature matrix\nclasses: array-like {n_samples}\n    List of class labels for prediction\nfeature_names: array-like {n_features} (default: None)\n    List of feature names as strings  Returns  None   tpot.predict(self, training_features, training_classes, testing_features)  Uses the optimized pipeline to predict the classes for a feature set.  Parameters  training_features: array-like {n_samples, n_features}\n    Feature matrix of the training set\ntraining_classes: array-like {n_samples}\n    List of class labels for prediction in the training set\ntesting_features: array-like {n_samples, n_features}\n    Feature matrix of the testing set  Returns  array-like: {n_samples}\n    Predicted classes for the testing set   tpot.score(self, training_features, training_classes, testing_features, testing_classes)  Estimates the testing accuracy of the optimized pipeline.  Parameters  training_features: array-like {n_samples, n_features}\n    Feature matrix of the training set\ntraining_classes: array-like {n_samples}\n    List of class labels for prediction in the training set\ntesting_features: array-like {n_samples, n_features}\n    Feature matrix of the testing set\ntesting_classes: array-like {n_samples}\n    List of class labels for prediction in the testing set  Returns  accuracy_score: float\n    The estimated test set accuracy   tpot.export(self, output_file_name)    Exports the current optimized pipeline as Python code.  Parameters  output_file_name: string\n    String containing the path and file name of the desired output file  Returns  None",
            "title": "TPOT"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/tree/DecisionTreeClassifier/",
            "text": "Decision Tree Classifier\n\n\n\n\nFits a Decision Tree classifier\n\n\nDependencies\n\n\nsklearn.tree.DecisionTreeClassifier\n\n\n\nParameters\n\n\ninput_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame for fitting the decision tree\nmax_features: int\n    Number of features used to fit the decision tree; must be a positive value\nmax_depth: int\n    Maximum depth of the decision tree; must be a positive value\n\n\n\nReturns\n\n\ninput_df: pandas.DataFrame {n_samples, n_features+['guess', 'group', 'class', 'SyntheticFeature']}\n    Returns a modified input DataFrame with the guess column updated according to the classifier's predictions.\n    Also adds the classifiers's predictions as a 'SyntheticFeature' column.\n\n\n\nExample Exported Code\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.tree import DecisionTreeClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\n# Perform classification with a decision tree classifier\nresult1 = tpot_data.copy()\n\ndtc1 = DecisionTreeClassifier(max_features='auto', max_depth=None)\ndtc1.fit(result1.loc[training_indices].drop('class', axis=1).values, result1.loc[training_indices, 'class'].values)\n\nresult1['dtc1-classification'] = dtc1.predict(result1.drop('class', axis=1).values)",
            "title": "DecisionTreeClassifier"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/tree/DecisionTreeClassifier/#decision-tree-classifier",
            "text": "Fits a Decision Tree classifier",
            "title": "Decision Tree Classifier"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/tree/DecisionTreeClassifier/#dependencies",
            "text": "sklearn.tree.DecisionTreeClassifier",
            "title": "Dependencies"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/tree/DecisionTreeClassifier/#parameters",
            "text": "input_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame for fitting the decision tree\nmax_features: int\n    Number of features used to fit the decision tree; must be a positive value\nmax_depth: int\n    Maximum depth of the decision tree; must be a positive value",
            "title": "Parameters"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/tree/DecisionTreeClassifier/#returns",
            "text": "input_df: pandas.DataFrame {n_samples, n_features+['guess', 'group', 'class', 'SyntheticFeature']}\n    Returns a modified input DataFrame with the guess column updated according to the classifier's predictions.\n    Also adds the classifiers's predictions as a 'SyntheticFeature' column.",
            "title": "Returns"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/tree/DecisionTreeClassifier/#example-exported-code",
            "text": "import numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.tree import DecisionTreeClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\n# Perform classification with a decision tree classifier\nresult1 = tpot_data.copy()\n\ndtc1 = DecisionTreeClassifier(max_features='auto', max_depth=None)\ndtc1.fit(result1.loc[training_indices].drop('class', axis=1).values, result1.loc[training_indices, 'class'].values)\n\nresult1['dtc1-classification'] = dtc1.predict(result1.drop('class', axis=1).values)",
            "title": "Example Exported Code"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/ensemble/RandomForestClassifier/",
            "text": "Random Forest Classifier\n\n\n\n\nFits a Random Forest classifier.\n\n\nDependencies\n\n\n sklearn.ensemble.RandomForestClassifier\n\n\n\nParameters\n\n\ninput_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame for fitting the random forest\nn_estimators: int\n    Number of trees in the random forest; must be a positive value\nmax_features: int\n    Number of features used to fit the decision tree; must be a positive value\n\n\n\nReturns\n\n\ninput_df: pandas.DataFrame {n_samples, n_features+['guess', 'group', 'class', 'SyntheticFeature']}\n    Returns a modified input DataFrame with the guess column updated according to the classifier's predictions.\n    Also adds the classifiers's predictions as a 'SyntheticFeature' column.\n\n\n\nExample Exported Code\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.ensemble import RandomForestClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\nresult1 = tpot_data.copy()\n\n# Perform classification with a random forest classifier\nrfc1 = RandomForestClassifier(n_estimators=1, max_features='auto')\nrfc1.fit(result1.loc[training_indices].drop('class', axis=1).values, result1.loc[training_indices, 'class'].values)\n\nresult1['rfc1-classification'] = rfc1.predict(result1.drop('class', axis=1).values)",
            "title": "RandomForestClassifier"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/ensemble/RandomForestClassifier/#random-forest-classifier",
            "text": "Fits a Random Forest classifier.",
            "title": "Random Forest Classifier"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/ensemble/RandomForestClassifier/#dependencies",
            "text": "sklearn.ensemble.RandomForestClassifier",
            "title": "Dependencies"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/ensemble/RandomForestClassifier/#parameters",
            "text": "input_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame for fitting the random forest\nn_estimators: int\n    Number of trees in the random forest; must be a positive value\nmax_features: int\n    Number of features used to fit the decision tree; must be a positive value",
            "title": "Parameters"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/ensemble/RandomForestClassifier/#returns",
            "text": "input_df: pandas.DataFrame {n_samples, n_features+['guess', 'group', 'class', 'SyntheticFeature']}\n    Returns a modified input DataFrame with the guess column updated according to the classifier's predictions.\n    Also adds the classifiers's predictions as a 'SyntheticFeature' column.",
            "title": "Returns"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/ensemble/RandomForestClassifier/#example-exported-code",
            "text": "import numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.ensemble import RandomForestClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\nresult1 = tpot_data.copy()\n\n# Perform classification with a random forest classifier\nrfc1 = RandomForestClassifier(n_estimators=1, max_features='auto')\nrfc1.fit(result1.loc[training_indices].drop('class', axis=1).values, result1.loc[training_indices, 'class'].values)\n\nresult1['rfc1-classification'] = rfc1.predict(result1.drop('class', axis=1).values)",
            "title": "Example Exported Code"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/ensemble/GradientBoostingClassifier/",
            "text": "Gradient Boosting Classifier\n\n\n\n\nFits a Gradient Boosting classifier.\n\n\nDependencies\n\n\nsklearn.ensemble.GradientBoostingClassifier\n\n\n\nParameters\n\n\ninput_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame for fitting the gradient boosting classifier\nlearning_rate: float\n    Shrinks the contribution of each tree by learning_rate\nn_estimators: int\n    The number of boosting stages to perform\nmax_depth: int\n    Maximum depth of the individual estimators; the maximum depth limits the number of nodes in the tree\n\n\n\nReturns\n\n\ninput_df: pandas.DataFrame {n_samples, n_features+['guess', 'group', 'class', 'SyntheticFeature']}\n    Returns a modified input DataFrame with the guess column updated according to the classifier's predictions.\n    Also adds the classifiers's predictions as a 'SyntheticFeature' column.\n\n\n\nExample Exported Code\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\nresult1 = tpot_data.copy()\n\n# Perform classification with a gradient boosting classifier\ngbc1 = GradientBoostingClassifier(learning_rate=0.0001, n_estimators=1, max_depth=None)\ngbc1.fit(result1.loc[training_indices].drop('class', axis=1).values, result1.loc[training_indices, 'class'].values)\n\nresult1['gbc1-classification'] = gbc1.predict(result1.drop('class', axis=1).values)",
            "title": "GradientBoostingClassifier"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/ensemble/GradientBoostingClassifier/#gradient-boosting-classifier",
            "text": "Fits a Gradient Boosting classifier.",
            "title": "Gradient Boosting Classifier"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/ensemble/GradientBoostingClassifier/#dependencies",
            "text": "sklearn.ensemble.GradientBoostingClassifier",
            "title": "Dependencies"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/ensemble/GradientBoostingClassifier/#parameters",
            "text": "input_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame for fitting the gradient boosting classifier\nlearning_rate: float\n    Shrinks the contribution of each tree by learning_rate\nn_estimators: int\n    The number of boosting stages to perform\nmax_depth: int\n    Maximum depth of the individual estimators; the maximum depth limits the number of nodes in the tree",
            "title": "Parameters"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/ensemble/GradientBoostingClassifier/#returns",
            "text": "input_df: pandas.DataFrame {n_samples, n_features+['guess', 'group', 'class', 'SyntheticFeature']}\n    Returns a modified input DataFrame with the guess column updated according to the classifier's predictions.\n    Also adds the classifiers's predictions as a 'SyntheticFeature' column.",
            "title": "Returns"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/ensemble/GradientBoostingClassifier/#example-exported-code",
            "text": "import numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\nresult1 = tpot_data.copy()\n\n# Perform classification with a gradient boosting classifier\ngbc1 = GradientBoostingClassifier(learning_rate=0.0001, n_estimators=1, max_depth=None)\ngbc1.fit(result1.loc[training_indices].drop('class', axis=1).values, result1.loc[training_indices, 'class'].values)\n\nresult1['gbc1-classification'] = gbc1.predict(result1.drop('class', axis=1).values)",
            "title": "Example Exported Code"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/svm/SVC/",
            "text": "C-Support Vector Classifier\n\n\n\n\nFits a C-Support Vector classifier.\n\n\nDependencies\n\n\nsklearn.svm.SVC\n\n\n\nParameters\n\n\ninput_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame for fitting the C-support vector classifier\nC: float\n    Penalty parameter C of the error term; must be a positive value\n\n\n\nReturns\n\n\ninput_df: pandas.DataFrame {n_samples, n_features+['guess', 'group', 'class', 'SyntheticFeature']}\n    Returns a modified input DataFrame with the guess column updated according to the classifier's predictions.\n    Also adds the classifiers's predictions as a 'SyntheticFeature' column.\n\n\n\nExample Exported Code\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.svm import SVC\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\nresult1 = tpot_data.copy()\n\n# Perform classification with a C-support vector classifier\nsvc1 = SVC(C=0.0001)\nsvc1.fit(result1.loc[training_indices].drop('class', axis=1).values, result1.loc[training_indices, 'class'].values)\n\nresult1['svc1-classification'] = svc1.predict(result1.drop('class', axis=1).values)",
            "title": "SVC"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/svm/SVC/#c-support-vector-classifier",
            "text": "Fits a C-Support Vector classifier.",
            "title": "C-Support Vector Classifier"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/svm/SVC/#dependencies",
            "text": "sklearn.svm.SVC",
            "title": "Dependencies"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/svm/SVC/#parameters",
            "text": "input_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame for fitting the C-support vector classifier\nC: float\n    Penalty parameter C of the error term; must be a positive value",
            "title": "Parameters"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/svm/SVC/#returns",
            "text": "input_df: pandas.DataFrame {n_samples, n_features+['guess', 'group', 'class', 'SyntheticFeature']}\n    Returns a modified input DataFrame with the guess column updated according to the classifier's predictions.\n    Also adds the classifiers's predictions as a 'SyntheticFeature' column.",
            "title": "Returns"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/svm/SVC/#example-exported-code",
            "text": "import numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.svm import SVC\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\nresult1 = tpot_data.copy()\n\n# Perform classification with a C-support vector classifier\nsvc1 = SVC(C=0.0001)\nsvc1.fit(result1.loc[training_indices].drop('class', axis=1).values, result1.loc[training_indices, 'class'].values)\n\nresult1['svc1-classification'] = svc1.predict(result1.drop('class', axis=1).values)",
            "title": "Example Exported Code"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/nearest_neighbors/kNeighborsClassifier/",
            "text": "k-Nearest Neighbors Classifier\n\n\n\n\nFits a k-Nearest Neighbors classifier.\n\n\nDependencies\n\n\nsklearn.neighbors.KNeighborsClassifier\n\n\n\nParameters\n\n\ninput_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame for fitting the k-nearest neighbor classifier\nn_neighbors: int\n    Number of neighbors to use by default for k_neighbors queries; must be a positive value\n\n\n\nReturns\n\n\ninput_df: pandas.DataFrame {n_samples, n_features+['guess', 'group', 'class', 'SyntheticFeature']}\n    Returns a modified input DataFrame with the guess column updated according to the classifier's predictions.\n    Also adds the classifiers's predictions as a 'SyntheticFeature' column.\n\n\n\nExample Exported Code\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\nresult1 = tpot_data.copy()\n\n# Perform classification with a k-nearest neighbor classifier\nknnc1 = KNeighborsClassifier(n_neighbors=2)\nknnc1.fit(result1.loc[training_indices].drop('class', axis=1).values, result1.loc[training_indices, 'class'].values)\nresult1['knnc1-classification'] = knnc1.predict(result1.drop('class', axis=1).values)",
            "title": "kNeighborsClassifier"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/nearest_neighbors/kNeighborsClassifier/#k-nearest-neighbors-classifier",
            "text": "Fits a k-Nearest Neighbors classifier.",
            "title": "k-Nearest Neighbors Classifier"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/nearest_neighbors/kNeighborsClassifier/#dependencies",
            "text": "sklearn.neighbors.KNeighborsClassifier",
            "title": "Dependencies"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/nearest_neighbors/kNeighborsClassifier/#parameters",
            "text": "input_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame for fitting the k-nearest neighbor classifier\nn_neighbors: int\n    Number of neighbors to use by default for k_neighbors queries; must be a positive value",
            "title": "Parameters"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/nearest_neighbors/kNeighborsClassifier/#returns",
            "text": "input_df: pandas.DataFrame {n_samples, n_features+['guess', 'group', 'class', 'SyntheticFeature']}\n    Returns a modified input DataFrame with the guess column updated according to the classifier's predictions.\n    Also adds the classifiers's predictions as a 'SyntheticFeature' column.",
            "title": "Returns"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/nearest_neighbors/kNeighborsClassifier/#example-exported-code",
            "text": "import numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\nresult1 = tpot_data.copy()\n\n# Perform classification with a k-nearest neighbor classifier\nknnc1 = KNeighborsClassifier(n_neighbors=2)\nknnc1.fit(result1.loc[training_indices].drop('class', axis=1).values, result1.loc[training_indices, 'class'].values)\nresult1['knnc1-classification'] = knnc1.predict(result1.drop('class', axis=1).values)",
            "title": "Example Exported Code"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/linear_model/LogisticRegression/",
            "text": "Logistic Regression\n\n\n\n\nFits a Logistic Regression classifier\n\n\nDependencies\n\n\nsklearn.linear_model.LogisticRegression\n\n\n\nParameters\n\n\ninput_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame for fitting the logistic regression classifier\nC: float\n    Inverse of regularization strength; must be a positive value. Like in support vector machines, smaller values specify stronger regularization.\n\n\n\nReturns\n\n\ninput_df: pandas.DataFrame {n_samples, n_features+['guess', 'group', 'class', 'SyntheticFeature']}\n    Returns a modified input DataFrame with the guess column updated according to the classifier's predictions.\n    Also adds the classifiers's predictions as a 'SyntheticFeature' column.\n\n\n\nExample Exported Code\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.linear_model import LogisticRegression\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\nresult1 = tpot_data.copy()\n\n# Perform classification with a logistic regression classifier\nlrc1 = LogisticRegression(C=0.0001)\nlrc1.fit(result1.loc[training_indices].drop('class', axis=1).values, result1.loc[training_indices, 'class'].values)\nresult1['lrc1-classification'] = lrc1.predict(result1.drop('class', axis=1).values)",
            "title": "LogisticRegression"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/linear_model/LogisticRegression/#logistic-regression",
            "text": "Fits a Logistic Regression classifier",
            "title": "Logistic Regression"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/linear_model/LogisticRegression/#dependencies",
            "text": "sklearn.linear_model.LogisticRegression",
            "title": "Dependencies"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/linear_model/LogisticRegression/#parameters",
            "text": "input_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame for fitting the logistic regression classifier\nC: float\n    Inverse of regularization strength; must be a positive value. Like in support vector machines, smaller values specify stronger regularization.",
            "title": "Parameters"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/linear_model/LogisticRegression/#returns",
            "text": "input_df: pandas.DataFrame {n_samples, n_features+['guess', 'group', 'class', 'SyntheticFeature']}\n    Returns a modified input DataFrame with the guess column updated according to the classifier's predictions.\n    Also adds the classifiers's predictions as a 'SyntheticFeature' column.",
            "title": "Returns"
        },
        {
            "location": "/reference/pipeline_operators/models/classifiers/linear_model/LogisticRegression/#example-exported-code",
            "text": "import numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.linear_model import LogisticRegression\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\nresult1 = tpot_data.copy()\n\n# Perform classification with a logistic regression classifier\nlrc1 = LogisticRegression(C=0.0001)\nlrc1.fit(result1.loc[training_indices].drop('class', axis=1).values, result1.loc[training_indices, 'class'].values)\nresult1['lrc1-classification'] = lrc1.predict(result1.drop('class', axis=1).values)",
            "title": "Example Exported Code"
        },
        {
            "location": "/reference/pipeline_operators/preprocessing/scaling/StandardScaler/",
            "text": "Standard Feature Scaler\n\n\n\n\nUses Scikit-learn's StandardScaler to scale the features by removing their mean and scaling to unit variance.\n\n\nDependencies\n\n\nsklearn.preprocessing.StandardScaler\n\n\n\nParameters\n\n\ninput_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame to scale\n\n\n\nReturns\n\n\nscaled_df: pandas.DataFrame {n_samples, n_features + ['guess', 'group', 'class']}\n    Returns a DataFrame containing the scaled features\n\n\n\nExample Exported Code\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\n# Use Scikit-learn's StandardScaler to scale the features\ntraining_features = tpot_data.loc[training_indices].drop('class', axis=1)\nresult1 = tpot_data.copy()\n\nif len(training_features.columns.values) > 0:\n    scaler = StandardScaler()\n    scaler.fit(training_features.values.astype(np.float64))\n    scaled_features = scaler.transform(result1.drop('class', axis=1).values.astype(np.float64))\n\n    for col_num, column in enumerate(result1.drop('class', axis=1).columns.values):\n        result1.loc[:, column] = scaled_features[:, col_num]\n\n\n# Perform classification with a decision tree classifier\nresult2 = result1.copy()\n\ndtc1 = DecisionTreeClassifier(max_features='auto', max_depth=None)\ndtc1.fit(result2.loc[training_indices].drop('class', axis=1).values, result2.loc[training_indices, 'class'].values)\n\nresult2['dtc1-classification'] = dtc1.predict(result2.drop('class', axis=1).values)",
            "title": "StandardScaler"
        },
        {
            "location": "/reference/pipeline_operators/preprocessing/scaling/StandardScaler/#standard-feature-scaler",
            "text": "Uses Scikit-learn's StandardScaler to scale the features by removing their mean and scaling to unit variance.",
            "title": "Standard Feature Scaler"
        },
        {
            "location": "/reference/pipeline_operators/preprocessing/scaling/StandardScaler/#dependencies",
            "text": "sklearn.preprocessing.StandardScaler",
            "title": "Dependencies"
        },
        {
            "location": "/reference/pipeline_operators/preprocessing/scaling/StandardScaler/#parameters",
            "text": "input_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame to scale",
            "title": "Parameters"
        },
        {
            "location": "/reference/pipeline_operators/preprocessing/scaling/StandardScaler/#returns",
            "text": "scaled_df: pandas.DataFrame {n_samples, n_features + ['guess', 'group', 'class']}\n    Returns a DataFrame containing the scaled features",
            "title": "Returns"
        },
        {
            "location": "/reference/pipeline_operators/preprocessing/scaling/StandardScaler/#example-exported-code",
            "text": "import numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\n# Use Scikit-learn's StandardScaler to scale the features\ntraining_features = tpot_data.loc[training_indices].drop('class', axis=1)\nresult1 = tpot_data.copy()\n\nif len(training_features.columns.values) > 0:\n    scaler = StandardScaler()\n    scaler.fit(training_features.values.astype(np.float64))\n    scaled_features = scaler.transform(result1.drop('class', axis=1).values.astype(np.float64))\n\n    for col_num, column in enumerate(result1.drop('class', axis=1).columns.values):\n        result1.loc[:, column] = scaled_features[:, col_num]\n\n\n# Perform classification with a decision tree classifier\nresult2 = result1.copy()\n\ndtc1 = DecisionTreeClassifier(max_features='auto', max_depth=None)\ndtc1.fit(result2.loc[training_indices].drop('class', axis=1).values, result2.loc[training_indices, 'class'].values)\n\nresult2['dtc1-classification'] = dtc1.predict(result2.drop('class', axis=1).values)",
            "title": "Example Exported Code"
        },
        {
            "location": "/reference/pipeline_operators/preprocessing/scaling/RobustScaler/",
            "text": "Robust Feature Scaler\n\n\n\n\nUses Scikit-learn's RobustScaler to scale the features using statistics that are robust to outliers.\n\n\nDependencies\n\n\nsklearn.preprocessing.RobustScaler\n\n\n\nParameters\n\n\ninput_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame to scale\n\n\n\nReturns\n\n\nscaled_df: pandas.DataFrame {n_samples, n_features + ['guess', 'group', 'class']}\n    Returns a DataFrame containing the scaled features\n\n\n\nExample Exported Code\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.tree import DecisionTreeClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\n# Use Scikit-learn's RobustScaler to scale the features\ntraining_features = tpot_data.loc[training_indices].drop('class', axis=1)\nresult1 = tpot_data.copy()\n\nif len(training_features.columns.values) > 0:\n    scaler = RobustScaler()\n    scaler.fit(training_features.values.astype(np.float64))\n    scaled_features = scaler.transform(result1.drop('class', axis=1).values.astype(np.float64))\n\n    for col_num, column in enumerate(result1.drop('class', axis=1).columns.values):\n        result1.loc[:, column] = scaled_features[:, col_num]\n\n# Perform classification with a decision tree classifier\nresult2 = result1.copy()\n\ndtc1 = DecisionTreeClassifier(max_features='auto', max_depth=None)\ndtc1.fit(result2.loc[training_indices].drop('class', axis=1).values, result2.loc[training_indices, 'class'].values)\n\nresult2['dtc1-classification'] = dtc1.predict(result2.drop('class', axis=1).values)",
            "title": "RobustScaler"
        },
        {
            "location": "/reference/pipeline_operators/preprocessing/scaling/RobustScaler/#robust-feature-scaler",
            "text": "Uses Scikit-learn's RobustScaler to scale the features using statistics that are robust to outliers.",
            "title": "Robust Feature Scaler"
        },
        {
            "location": "/reference/pipeline_operators/preprocessing/scaling/RobustScaler/#dependencies",
            "text": "sklearn.preprocessing.RobustScaler",
            "title": "Dependencies"
        },
        {
            "location": "/reference/pipeline_operators/preprocessing/scaling/RobustScaler/#parameters",
            "text": "input_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame to scale",
            "title": "Parameters"
        },
        {
            "location": "/reference/pipeline_operators/preprocessing/scaling/RobustScaler/#returns",
            "text": "scaled_df: pandas.DataFrame {n_samples, n_features + ['guess', 'group', 'class']}\n    Returns a DataFrame containing the scaled features",
            "title": "Returns"
        },
        {
            "location": "/reference/pipeline_operators/preprocessing/scaling/RobustScaler/#example-exported-code",
            "text": "import numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.tree import DecisionTreeClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\n# Use Scikit-learn's RobustScaler to scale the features\ntraining_features = tpot_data.loc[training_indices].drop('class', axis=1)\nresult1 = tpot_data.copy()\n\nif len(training_features.columns.values) > 0:\n    scaler = RobustScaler()\n    scaler.fit(training_features.values.astype(np.float64))\n    scaled_features = scaler.transform(result1.drop('class', axis=1).values.astype(np.float64))\n\n    for col_num, column in enumerate(result1.drop('class', axis=1).columns.values):\n        result1.loc[:, column] = scaled_features[:, col_num]\n\n# Perform classification with a decision tree classifier\nresult2 = result1.copy()\n\ndtc1 = DecisionTreeClassifier(max_features='auto', max_depth=None)\ndtc1.fit(result2.loc[training_indices].drop('class', axis=1).values, result2.loc[training_indices, 'class'].values)\n\nresult2['dtc1-classification'] = dtc1.predict(result2.drop('class', axis=1).values)",
            "title": "Example Exported Code"
        },
        {
            "location": "/reference/pipeline_operators/preprocessing/PolynomialFeatures/",
            "text": "Polynomial Features\n\n\n\n\nUses Scikit-learn's PolynomialFeatures to construct new degree-2 polynomial features from the existing feature set.\n\n\nDependencies\n\n\nsklearn.preprocessing.PolynomialFeatures\n\n\n\nParameters\n\n\ninput_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame to scale\n\n\n\nReturns\n\n\nmodified_df: pandas.DataFrame {n_samples, n_constructed_features + ['guess', 'group', 'class']}\n    Returns a DataFrame containing the constructed features\n\n\n\nExample Exported Code\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.tree import DecisionTreeClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\n# Use Scikit-learn's PolynomialFeatures to construct new features from the existing feature set\ntraining_features = tpot_data.loc[training_indices].drop('class', axis=1)\n\nif len(training_features.columns.values) > 0 and len(training_features.columns.values) <= 700:\n    # The feature constructor must be fit on only the training data\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    poly.fit(training_features.values.astype(np.float64))\n    constructed_features = poly.transform(tpot_data.drop('class', axis=1).values.astype(np.float64))\n\n    tpot_data_classes = tpot_data['class'].values\n    result1 = pd.DataFrame(data=constructed_features)\n    result1['class'] = tpot_data_classes\nelse:\n    result1 = tpot_data.copy()\n\n# Perform classification with a decision tree classifier\nresult2 = result1.copy()\n\ndtc1 = DecisionTreeClassifier(max_features='auto', max_depth=None)\ndtc1.fit(result2.loc[training_indices].drop('class', axis=1).values, result2.loc[training_indices, 'class'].values)\n\nresult2['dtc1-classification'] = dtc1.predict(result2.drop('class', axis=1).values)",
            "title": "PolynomialFeatures"
        },
        {
            "location": "/reference/pipeline_operators/preprocessing/PolynomialFeatures/#polynomial-features",
            "text": "Uses Scikit-learn's PolynomialFeatures to construct new degree-2 polynomial features from the existing feature set.",
            "title": "Polynomial Features"
        },
        {
            "location": "/reference/pipeline_operators/preprocessing/PolynomialFeatures/#dependencies",
            "text": "sklearn.preprocessing.PolynomialFeatures",
            "title": "Dependencies"
        },
        {
            "location": "/reference/pipeline_operators/preprocessing/PolynomialFeatures/#parameters",
            "text": "input_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame to scale",
            "title": "Parameters"
        },
        {
            "location": "/reference/pipeline_operators/preprocessing/PolynomialFeatures/#returns",
            "text": "modified_df: pandas.DataFrame {n_samples, n_constructed_features + ['guess', 'group', 'class']}\n    Returns a DataFrame containing the constructed features",
            "title": "Returns"
        },
        {
            "location": "/reference/pipeline_operators/preprocessing/PolynomialFeatures/#example-exported-code",
            "text": "import numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.tree import DecisionTreeClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\n# Use Scikit-learn's PolynomialFeatures to construct new features from the existing feature set\ntraining_features = tpot_data.loc[training_indices].drop('class', axis=1)\n\nif len(training_features.columns.values) > 0 and len(training_features.columns.values) <= 700:\n    # The feature constructor must be fit on only the training data\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    poly.fit(training_features.values.astype(np.float64))\n    constructed_features = poly.transform(tpot_data.drop('class', axis=1).values.astype(np.float64))\n\n    tpot_data_classes = tpot_data['class'].values\n    result1 = pd.DataFrame(data=constructed_features)\n    result1['class'] = tpot_data_classes\nelse:\n    result1 = tpot_data.copy()\n\n# Perform classification with a decision tree classifier\nresult2 = result1.copy()\n\ndtc1 = DecisionTreeClassifier(max_features='auto', max_depth=None)\ndtc1.fit(result2.loc[training_indices].drop('class', axis=1).values, result2.loc[training_indices, 'class'].values)\n\nresult2['dtc1-classification'] = dtc1.predict(result2.drop('class', axis=1).values)",
            "title": "Example Exported Code"
        },
        {
            "location": "/reference/pipeline_operators/feature_selection/VarianceThreshold/",
            "text": "Variance Threshold\n\n\n\n\nUses Scikit-learn's VarianceThreshold feature selection to learn the subset of features that pass the variance threshold. \n\n\nDependencies\n\n\nsklearn.feature_selection.VarianceThreshold\n\n\n\nParameters\n\n\ninput_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame to perform feature selection on\nthreshold: float\n    The variance threshold that removes features that fall under the threshold\n\n\n\nReturns\n\n\nsubsetted_df: pandas.DataFrame {n_samples, n_filtered_features + ['guess', 'group', 'class']}\n    Returns a DataFrame containing the features that are above the variance threshold\n\n\n\nExample Exported Code\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.tree import DecisionTreeClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\n# Use Scikit-learn's VarianceThreshold for feature selection\ntraining_features = tpot_data.loc[training_indices].drop('class', axis=1)\n\nselector = VarianceThreshold(threshold=0.50)\ntry:\n    selector.fit(training_features.values)\nexcept ValueError:\n    # None of the features meet the variance threshold\n    result1 = tpot_data[['class']]\n\nmask = selector.get_support(True)\nmask_cols = list(training_features.iloc[:, mask].columns) + ['class']\nresult1 = tpot_data[mask_cols]\n\n# Perform classification with a decision tree classifier\nresult2 = result1.copy()\n\ndtc1 = DecisionTreeClassifier(max_features='auto', max_depth=None)\ndtc1.fit(result2.loc[training_indices].drop('class', axis=1).values, result2.loc[training_indices, 'class'].values)\n\nresult2['dtc1-classification'] = dtc1.predict(result2.drop('class', axis=1).values)",
            "title": "VarianceThreshold"
        },
        {
            "location": "/reference/pipeline_operators/feature_selection/VarianceThreshold/#variance-threshold",
            "text": "Uses Scikit-learn's VarianceThreshold feature selection to learn the subset of features that pass the variance threshold.",
            "title": "Variance Threshold"
        },
        {
            "location": "/reference/pipeline_operators/feature_selection/VarianceThreshold/#dependencies",
            "text": "sklearn.feature_selection.VarianceThreshold",
            "title": "Dependencies"
        },
        {
            "location": "/reference/pipeline_operators/feature_selection/VarianceThreshold/#parameters",
            "text": "input_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame to perform feature selection on\nthreshold: float\n    The variance threshold that removes features that fall under the threshold",
            "title": "Parameters"
        },
        {
            "location": "/reference/pipeline_operators/feature_selection/VarianceThreshold/#returns",
            "text": "subsetted_df: pandas.DataFrame {n_samples, n_filtered_features + ['guess', 'group', 'class']}\n    Returns a DataFrame containing the features that are above the variance threshold",
            "title": "Returns"
        },
        {
            "location": "/reference/pipeline_operators/feature_selection/VarianceThreshold/#example-exported-code",
            "text": "import numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.tree import DecisionTreeClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\n# Use Scikit-learn's VarianceThreshold for feature selection\ntraining_features = tpot_data.loc[training_indices].drop('class', axis=1)\n\nselector = VarianceThreshold(threshold=0.50)\ntry:\n    selector.fit(training_features.values)\nexcept ValueError:\n    # None of the features meet the variance threshold\n    result1 = tpot_data[['class']]\n\nmask = selector.get_support(True)\nmask_cols = list(training_features.iloc[:, mask].columns) + ['class']\nresult1 = tpot_data[mask_cols]\n\n# Perform classification with a decision tree classifier\nresult2 = result1.copy()\n\ndtc1 = DecisionTreeClassifier(max_features='auto', max_depth=None)\ndtc1.fit(result2.loc[training_indices].drop('class', axis=1).values, result2.loc[training_indices, 'class'].values)\n\nresult2['dtc1-classification'] = dtc1.predict(result2.drop('class', axis=1).values)",
            "title": "Example Exported Code"
        },
        {
            "location": "/reference/pipeline_operators/feature_selection/SelectKBest/",
            "text": "Select K-Best\n\n\n\n\nUses Scikit-learn's SelectKBest feature selection to learn the subset of features that have the highest score according to some scoring function.\n\n\nDependencies\n\n\nsklearn.feature_selection.SelectKBest\nsklearn.feature_selection.f_classif\n\n\n\nParameters\n\n\ninput_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame to perform feature selection on\nk: int\n    The top k features to keep from the original set of features in the training data\n\n\n\nReturns\n\n\nsubsetted_df: pandas.DataFrame {n_samples, n_filtered_features + ['guess', 'group', 'class']}\n    Returns a DataFrame containing the `k` best features\n\n\n\nExample Exported Code\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.tree import DecisionTreeClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\n# Use Scikit-learn's SelectKBest for feature selection\ntraining_features = tpot_data.loc[training_indices].drop('class', axis=1)\ntraining_class_vals = tpot_data.loc[training_indices, 'class'].values\n\nif len(training_features.columns.values) == 0:\n    result1 = tpot_data.copy()\nelse:\n    selector = SelectKBest(f_classif, k=1)\n    selector.fit(training_features.values, training_class_vals)\n    mask = selector.get_support(True)\n    mask_cols = list(training_features.iloc[:, mask].columns) + ['class']\n    result1 = tpot_data[mask_cols]\n\n\n# Perform classification with a decision tree classifier\nresult2 = result1.copy()\n\ndtc1 = DecisionTreeClassifier(max_features='auto', max_depth=None)\ndtc1.fit(result2.loc[training_indices].drop('class', axis=1).values, result2.loc[training_indices, 'class'].values)\n\nresult2['dtc1-classification'] = dtc1.predict(result2.drop('class', axis=1).values)",
            "title": "SelectKBest"
        },
        {
            "location": "/reference/pipeline_operators/feature_selection/SelectKBest/#select-k-best",
            "text": "Uses Scikit-learn's SelectKBest feature selection to learn the subset of features that have the highest score according to some scoring function.",
            "title": "Select K-Best"
        },
        {
            "location": "/reference/pipeline_operators/feature_selection/SelectKBest/#dependencies",
            "text": "sklearn.feature_selection.SelectKBest\nsklearn.feature_selection.f_classif",
            "title": "Dependencies"
        },
        {
            "location": "/reference/pipeline_operators/feature_selection/SelectKBest/#parameters",
            "text": "input_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame to perform feature selection on\nk: int\n    The top k features to keep from the original set of features in the training data",
            "title": "Parameters"
        },
        {
            "location": "/reference/pipeline_operators/feature_selection/SelectKBest/#returns",
            "text": "subsetted_df: pandas.DataFrame {n_samples, n_filtered_features + ['guess', 'group', 'class']}\n    Returns a DataFrame containing the `k` best features",
            "title": "Returns"
        },
        {
            "location": "/reference/pipeline_operators/feature_selection/SelectKBest/#example-exported-code",
            "text": "import numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.tree import DecisionTreeClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\n# Use Scikit-learn's SelectKBest for feature selection\ntraining_features = tpot_data.loc[training_indices].drop('class', axis=1)\ntraining_class_vals = tpot_data.loc[training_indices, 'class'].values\n\nif len(training_features.columns.values) == 0:\n    result1 = tpot_data.copy()\nelse:\n    selector = SelectKBest(f_classif, k=1)\n    selector.fit(training_features.values, training_class_vals)\n    mask = selector.get_support(True)\n    mask_cols = list(training_features.iloc[:, mask].columns) + ['class']\n    result1 = tpot_data[mask_cols]\n\n\n# Perform classification with a decision tree classifier\nresult2 = result1.copy()\n\ndtc1 = DecisionTreeClassifier(max_features='auto', max_depth=None)\ndtc1.fit(result2.loc[training_indices].drop('class', axis=1).values, result2.loc[training_indices, 'class'].values)\n\nresult2['dtc1-classification'] = dtc1.predict(result2.drop('class', axis=1).values)",
            "title": "Example Exported Code"
        },
        {
            "location": "/reference/pipeline_operators/feature_selection/SelectPercentile/",
            "text": "Select Percentile\n\n\n\n\nUses Scikit-learn's SelectPercentile feature selection to learn the subset of features that belong in the highest \npercentile\n according to a given scoring function..\n\n\nDependencies\n\n\nsklearn.feature_selection.SelectPercentile\nsklearn.feature_selection.f_classif\n\n\n\nParameters\n\n\ninput_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame to perform feature selection on\npercentile: int\n    The features that belong in the top percentile to keep from the original set of features in the training data\n\n\n\nReturns\n\n\nsubsetted_df: pandas.DataFrame {n_samples, n_filtered_features + ['guess', 'group', 'class']}\n    Returns a DataFrame containing the best features in the given `percentile`\n\n\n\nExample Exported Code\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.tree import DecisionTreeClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\n\n# Use Scikit-learn's SelectPercentile for feature selection\ntraining_features = tpot_data.loc[training_indices].drop('class', axis=1)\ntraining_class_vals = tpot_data.loc[training_indices, 'class'].values\n\nif len(training_features.columns.values) == 0:\n    result1 = tpot_data.copy()\nelse:\n    selector = SelectPercentile(f_classif, percentile=100)\n    selector.fit(training_features.values, training_class_vals)\n    mask = selector.get_support(True)\n    mask_cols = list(training_features.iloc[:, mask].columns) + ['class']\n    result1 = tpot_data[mask_cols]\n\n\n# Perform classification with a decision tree classifier\nresult2 = result1.copy()\n\ndtc1 = DecisionTreeClassifier(max_features='auto', max_depth=None)\ndtc1.fit(result2.loc[training_indices].drop('class', axis=1).values, result2.loc[training_indices, 'class'].values)\n\nresult2['dtc1-classification'] = dtc1.predict(result2.drop('class', axis=1).values)",
            "title": "SelectPercentile"
        },
        {
            "location": "/reference/pipeline_operators/feature_selection/SelectPercentile/#select-percentile",
            "text": "Uses Scikit-learn's SelectPercentile feature selection to learn the subset of features that belong in the highest  percentile  according to a given scoring function..",
            "title": "Select Percentile"
        },
        {
            "location": "/reference/pipeline_operators/feature_selection/SelectPercentile/#dependencies",
            "text": "sklearn.feature_selection.SelectPercentile\nsklearn.feature_selection.f_classif",
            "title": "Dependencies"
        },
        {
            "location": "/reference/pipeline_operators/feature_selection/SelectPercentile/#parameters",
            "text": "input_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame to perform feature selection on\npercentile: int\n    The features that belong in the top percentile to keep from the original set of features in the training data",
            "title": "Parameters"
        },
        {
            "location": "/reference/pipeline_operators/feature_selection/SelectPercentile/#returns",
            "text": "subsetted_df: pandas.DataFrame {n_samples, n_filtered_features + ['guess', 'group', 'class']}\n    Returns a DataFrame containing the best features in the given `percentile`",
            "title": "Returns"
        },
        {
            "location": "/reference/pipeline_operators/feature_selection/SelectPercentile/#example-exported-code",
            "text": "import numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.tree import DecisionTreeClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\n\n# Use Scikit-learn's SelectPercentile for feature selection\ntraining_features = tpot_data.loc[training_indices].drop('class', axis=1)\ntraining_class_vals = tpot_data.loc[training_indices, 'class'].values\n\nif len(training_features.columns.values) == 0:\n    result1 = tpot_data.copy()\nelse:\n    selector = SelectPercentile(f_classif, percentile=100)\n    selector.fit(training_features.values, training_class_vals)\n    mask = selector.get_support(True)\n    mask_cols = list(training_features.iloc[:, mask].columns) + ['class']\n    result1 = tpot_data[mask_cols]\n\n\n# Perform classification with a decision tree classifier\nresult2 = result1.copy()\n\ndtc1 = DecisionTreeClassifier(max_features='auto', max_depth=None)\ndtc1.fit(result2.loc[training_indices].drop('class', axis=1).values, result2.loc[training_indices, 'class'].values)\n\nresult2['dtc1-classification'] = dtc1.predict(result2.drop('class', axis=1).values)",
            "title": "Example Exported Code"
        },
        {
            "location": "/reference/pipeline_operators/feature_selection/RFE/",
            "text": "Recursive Feature Elimination\n\n\n\n\nUses Scikit-learn's Recursive Feature Elimintation to learn the subset of features that have the highest weights according to the estimator.\n\n\nDependencies\n\n\nsklearn.feature_selection.RFE\nsklearn.svm.SVC\n\n\n\nParameters\n\n\ninput_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame to perform feature selection on\nnum_features: int\n    The number of features to select\nstep: float\n    The percentage of features to drop each iteration\n\n\n\nReturns\n\n\nsubsetted_df: pandas.DataFrame {n_samples, n_filtered_features + ['guess', 'group', 'class']}\n    Returns a DataFrame containing the `num_features` best features\n\n\n\nExample Exported Code\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.feature_selection import RFE\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\n\n# Use Scikit-learn's Recursive Feature Elimination (RFE) for feature selection\ntraining_features = tpot_data.loc[training_indices].drop('class', axis=1)\ntraining_class_vals = tpot_data.loc[training_indices, 'class'].values\n\nif len(training_features.columns.values) == 0:\n    result1 = tpot_data.copy()\nelse:\n    selector = RFE(SVC(kernel='linear'), n_features_to_select=1, step=0.99)\n    selector.fit(training_features.values, training_class_vals)\n    mask = selector.get_support(True)\n    mask_cols = list(training_features.iloc[:, mask].columns) + ['class']\n    result1 = tpot_data[mask_cols]\n\n# Perform classification with a decision tree classifier\nresult2 = result1.copy()\n\ndtc1 = DecisionTreeClassifier(max_features='auto', max_depth=None)\ndtc1.fit(result2.loc[training_indices].drop('class', axis=1).values, result2.loc[training_indices, 'class'].values)\n\nresult2['dtc1-classification'] = dtc1.predict(result2.drop('class', axis=1).values)",
            "title": "RFE"
        },
        {
            "location": "/reference/pipeline_operators/feature_selection/RFE/#recursive-feature-elimination",
            "text": "Uses Scikit-learn's Recursive Feature Elimintation to learn the subset of features that have the highest weights according to the estimator.",
            "title": "Recursive Feature Elimination"
        },
        {
            "location": "/reference/pipeline_operators/feature_selection/RFE/#dependencies",
            "text": "sklearn.feature_selection.RFE\nsklearn.svm.SVC",
            "title": "Dependencies"
        },
        {
            "location": "/reference/pipeline_operators/feature_selection/RFE/#parameters",
            "text": "input_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame to perform feature selection on\nnum_features: int\n    The number of features to select\nstep: float\n    The percentage of features to drop each iteration",
            "title": "Parameters"
        },
        {
            "location": "/reference/pipeline_operators/feature_selection/RFE/#returns",
            "text": "subsetted_df: pandas.DataFrame {n_samples, n_filtered_features + ['guess', 'group', 'class']}\n    Returns a DataFrame containing the `num_features` best features",
            "title": "Returns"
        },
        {
            "location": "/reference/pipeline_operators/feature_selection/RFE/#example-exported-code",
            "text": "import numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.feature_selection import RFE\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\n\n# Use Scikit-learn's Recursive Feature Elimination (RFE) for feature selection\ntraining_features = tpot_data.loc[training_indices].drop('class', axis=1)\ntraining_class_vals = tpot_data.loc[training_indices, 'class'].values\n\nif len(training_features.columns.values) == 0:\n    result1 = tpot_data.copy()\nelse:\n    selector = RFE(SVC(kernel='linear'), n_features_to_select=1, step=0.99)\n    selector.fit(training_features.values, training_class_vals)\n    mask = selector.get_support(True)\n    mask_cols = list(training_features.iloc[:, mask].columns) + ['class']\n    result1 = tpot_data[mask_cols]\n\n# Perform classification with a decision tree classifier\nresult2 = result1.copy()\n\ndtc1 = DecisionTreeClassifier(max_features='auto', max_depth=None)\ndtc1.fit(result2.loc[training_indices].drop('class', axis=1).values, result2.loc[training_indices, 'class'].values)\n\nresult2['dtc1-classification'] = dtc1.predict(result2.drop('class', axis=1).values)",
            "title": "Example Exported Code"
        },
        {
            "location": "/reference/pipeline_operators/decomposition/RandomizedPCA/",
            "text": "Randomized Principal Component Analysis\n\n\n\n\nUses Scikit-learn's RandomizedPCA to transform the feature set.\n\n\nDependencies\n\n\nsklearn.decomposition.RandomizedPCA\n\n\n\nParameters\n\n\ninput_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame to scale\nn_components: int\n    The number of components to keep\niterated_power: int\n    Number of iterations for the power method. [1, 10]\n\n\n\nReturns\n\n\nmodified_df: pandas.DataFrame {n_samples, n_components + ['guess', 'group', 'class']}\n    Returns a DataFrame containing the transformed features\n\n\n\nExample Exported Code\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.decomposition import RandomizedPCA\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\n\n# Use Scikit-learn's RandomizedPCA to transform the feature set\ntraining_features = tpot_data.loc[training_indices].drop('class', axis=1)\n\nif len(training_features.columns.values) > 0:\n    # RandomizedPCA must be fit on only the training data\n    pca = RandomizedPCA(n_components=1, iterated_power=10)\n    pca.fit(training_features.values.astype(np.float64))\n    transformed_features = pca.transform(tpot_data.drop('class', axis=1).values.astype(np.float64))\n\n    tpot_data_classes = tpot_data['class'].values\n    result1 = pd.DataFrame(data=transformed_features)\n    result1['class'] = tpot_data_classes\nelse:\n    result1 = tpot_data.copy()\n\n# Perform classification with a decision tree classifier\nresult2 = result1.copy()\n\ndtc1 = DecisionTreeClassifier(max_features='auto', max_depth=None)\ndtc1.fit(result2.loc[training_indices].drop('class', axis=1).values, result2.loc[training_indices, 'class'].values)\n\nresult2['dtc1-classification'] = dtc1.predict(result2.drop('class', axis=1).values)",
            "title": "RandomizedPCA"
        },
        {
            "location": "/reference/pipeline_operators/decomposition/RandomizedPCA/#randomized-principal-component-analysis",
            "text": "Uses Scikit-learn's RandomizedPCA to transform the feature set.",
            "title": "Randomized Principal Component Analysis"
        },
        {
            "location": "/reference/pipeline_operators/decomposition/RandomizedPCA/#dependencies",
            "text": "sklearn.decomposition.RandomizedPCA",
            "title": "Dependencies"
        },
        {
            "location": "/reference/pipeline_operators/decomposition/RandomizedPCA/#parameters",
            "text": "input_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}\n    Input DataFrame to scale\nn_components: int\n    The number of components to keep\niterated_power: int\n    Number of iterations for the power method. [1, 10]",
            "title": "Parameters"
        },
        {
            "location": "/reference/pipeline_operators/decomposition/RandomizedPCA/#returns",
            "text": "modified_df: pandas.DataFrame {n_samples, n_components + ['guess', 'group', 'class']}\n    Returns a DataFrame containing the transformed features",
            "title": "Returns"
        },
        {
            "location": "/reference/pipeline_operators/decomposition/RandomizedPCA/#example-exported-code",
            "text": "import numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.decomposition import RandomizedPCA\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\ntraining_indices, testing_indices = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))\n\n\n# Use Scikit-learn's RandomizedPCA to transform the feature set\ntraining_features = tpot_data.loc[training_indices].drop('class', axis=1)\n\nif len(training_features.columns.values) > 0:\n    # RandomizedPCA must be fit on only the training data\n    pca = RandomizedPCA(n_components=1, iterated_power=10)\n    pca.fit(training_features.values.astype(np.float64))\n    transformed_features = pca.transform(tpot_data.drop('class', axis=1).values.astype(np.float64))\n\n    tpot_data_classes = tpot_data['class'].values\n    result1 = pd.DataFrame(data=transformed_features)\n    result1['class'] = tpot_data_classes\nelse:\n    result1 = tpot_data.copy()\n\n# Perform classification with a decision tree classifier\nresult2 = result1.copy()\n\ndtc1 = DecisionTreeClassifier(max_features='auto', max_depth=None)\ndtc1.fit(result2.loc[training_indices].drop('class', axis=1).values, result2.loc[training_indices, 'class'].values)\n\nresult2['dtc1-classification'] = dtc1.predict(result2.drop('class', axis=1).values)",
            "title": "Example Exported Code"
        },
        {
            "location": "/changelog/",
            "text": "Changelog\n\n\nv0.1.3\n\n\n\n\nFirst Release\n\n\nDownload links: \nSource code (zip)\n, \nSource code (tar.gz)",
            "title": "Changelog"
        },
        {
            "location": "/changelog/#changelog",
            "text": "",
            "title": "Changelog"
        },
        {
            "location": "/changelog/#v013",
            "text": "First Release  Download links:  Source code (zip) ,  Source code (tar.gz)",
            "title": "v0.1.3"
        }
    ]
}